---
title: Intelligence artificielle : brève histoire et tentative de définitions
tags: [IA]
last_updated: July 3, 2016
keywords: IA, IT
summary: "Brève histoire et définitions de l'IA"
sidebar: mydoc_sidebar
permalink: IA_histoire.html
folder: mydoc
---

2.	L’intelligence artificielle

2.1.	Historique

L'essor récent de ChatGPT tend à éclipser le riche historique de l’intelligence artificielle (IA). En effet, on peut faire remonter son émergence au milieu du siècle passé déjà voire aux premiers calculateurs programmables tels Enigma. Si les premiers travaux sur cette thématique datent de 1943 (article de Mc Culloch et Pits sur le neurone formel) et que la cybernétique n'attend pas 1950 pour se développer, c'est à partir de 1956 et la conférence de Dartmouth que l'IA est reconnue comme une discipline de recherche à part entière (Rodriguez 2022, p. 41 ; Vannieuwenh 2019, p. 24}). Pendant les premières décennies de l'après-guerre, les travaux sur le thème sont nombreux (traduction automatique et étude du langage naturel dès les années 50, perceptrons dès 1957, algorithmes génétiques dès 1960, etc.), propulsés par la mise au point du langage Lisp qui ouvre la porte à la programmation fonctionnelle (Frécon et Kazar 2009, p. 3).
En raison de limites autant scientifiques que techniques, plusieurs projets d'IA sont toutefois mis en échec entre 1970 et 1980, de quoi provoquer un certain désenchantement et tarir les financements vers ce domaine, malgré certains succès des modèles connexionnistes. Ces années austères sont souvent désignées comme le premier hiver de l'IA (Vannieuwenh 2019, p. 26 ; Azencott 2019, p. 98).
La discipline rebondit rapidement puisque les premières années de la décennie 1980 sont marquées par une recherche très dynamique conduisant à l'essor et aux premiers succès commerciaux des systèmes experts (Rodriguez 2022, p. 43). Ces programmes sont conçus pour répondre à des questions en se basant sur des règles et des faits spécifiés à l'avance. Ils se limitent cependant à des champs d'expertise précis et sont difficiles à maintenir sur le long terme. Ces restrictions, ainsi que l'avènement d'autres paradigmes (les machines à vecteur de support, par exemple ; Gallego et Rìos Insua 2022), freinent l'engouement et l'IA connait ainsi son deuxième hiver au crépuscule du 19ème millénaire (Vannieuwenh 2019, p. 26}).
Une nouvelle fois, l'IA se relève et connait un nouvel âge d'or dès les premières décennies des années 2000. En effet, le perfectionnement des algorithmes, la sophistication des techniques d'entrainement des réseaux de neurones multicouches et de leur architecture, une puissance de calcul accrue des ordinateurs (hardware) ainsi qu'une disponibilité accrue de données (Internet) concourent à replacer l'IA au premier plan (i.a. Gallego et Rìos Insua 2022 ; Yvon 2022). Avec la mise à disposition de librairies accessibles comme TensorFlow ou Theano, la discipline gagne en attractivité et devient le terrain de jeu des ingénieurs (Arkhangelskaya et Nikolenko 2023). Parmi les étapes marquantes dans le développement de ces technologies, l'on peut notamment citer le super-calculateur d'IBM en 2011, DeepFace de Facebook en 2014, ou Alphago qui met en échec l'un des meilleurs joueurs de go deux ans plus tard. Avec Alexa (Amazon), l'IA devient largement accessible au grand public en 2019 (Vannieuwenh 2019, p. 26}).
Confinée aux laboratoires à ses prémices, l'IA a progressivement infiltré de nombreux aspects de notre vie quotidienne. Des usages "dissimulés" (fonctionnement des véhicules, d'appareils ménagers, etc.) à son intégration dans des applications en vogue (filtres Instagram, édition d'images sur Snapchat), l'IA est aujourd’hui omniprésente et rend bien des services. Selon les estimations, une personne utilise de l’IA plus de 200 fois par jour, un chiffre attendu à près de 5'000 d’ici 2025 (https://www.kmu.admin.ch/kmu/fr/home/faits-et-tendances/intelligence-artificielle.html). Dans l'administration cantonale du Valais, bilingue, l'outil DeepL par exemple - certainement utilisée plusieurs fois par jour par l'ensemble des collaborateurs - a même intégré les applications "maison". 

2.2.	Quelques (tentatives de) définitions

2.2.1.	Intelligence artificielle
Le concept d'intelligence en soi ne se laissant pas aisément appréhender, lui adjoindre le qualificatif "artificielle" ne simplifie en rien la tâche de la définir. Avant de s’y aventurer, il n'est pas vain de rappeler que cet adjectif ne doit pas dissimuler l'intervention de l'humain à chaque étape de sa mise au point (choix des données, écriture de l'algorithme, etc.). En cela, l'IA est bien une « techniqu[e] humain[e] [...] "technique" renvoyant à un "ensemble de moyens, convenablement ordonnés, qui permettent d'atteindre une fin désirable" » (Linden et al. 2023, citant Gaston Berger). 
Bien que l’IA fasse régulièrement la une de la presse, elle est souvent mal comprise. Cela peut s’expliquer par la grande diversité des définitions à travers le temps, mais également, car aucune définition n’est communément acceptée (Baudet et al. 2023). Pour les quatre protagonistes (John McCarthy, Marvin Minsky, Nathaniel Rochester et Claude Shannon) à l'initiative de l'université d'été de Darmouth, l'IA repose sur l'hypothèse que « toutes les fonctions cognitives humaines peuvent être décrites de façon très précise pouvant alors donner lieu à une reproduction de celles-ci sur ordinateur » (Vannieuwenh 2019, p. 24). Les termes « intelligence artificielle » avaient alors été choisis dans un sens plutôt métaphorique (Linden et al. 2023). Le choix du mot « intelligence », perçu généralement de façon positive, revêt d’emblée un caractère connoté et normatif (Byk et Piana 2021).
Brièvement résumée à l'"art de simuler l'intelligence à l'aide d'ordinateurs", Frécon et Kazar la posent comme discipline sœur de la psychologie cognitive (Frécon et Kazar 2009, p. 2). Vannieuwenh (2019) qualifie l'IA de "mot valise", rappelant la définition de Marvin Minsky la décrivant comme la « science dont le but est de faire réaliser par une machine des tâches que l'Homme accomplit en utilisant son intelligence » (Vannieuwenh 2019, p. 28). De façon presque aussi vague, Azencott (2019, p. 4) la définit comme « l'ensemble des techniques mises en œuvre afin de construire des machines capables de faire preuve d’un comportement que l’on peut qualifier d’intelligent, [faisant] appel aux sciences cognitives, à la neurobiologie, à la logique, à l’électronique, à l’ingénierie et bien plus encore ». Formulé de façon similaire, le vocable regroupe donc « un ensemble de théories, d’algorithmes et de logiciels qui ont pour objectif de simuler certaines capacités cognitives de l’être humain, comme les capacités d’apprentissage, de perception, d’aide à la décision et de génération » (Courtier-Orgogozo et Devillers 2024) ou, plus simplement, un ensemble des systèmes qui pensent ou agissent comme les humains ou de manière rationnelle (Rodriguez 2022, p. 40). Certaines définitions sont plus restrictives, à l’instar de celle de Kaplan et Haenlein (2019 ; cité dans Baudet et al. 2023) qui perçoivent l’IA comme « la capacité d'un système informatique à interpréter correctement des données externes, à apprendre à partir de ces données et à utiliser ces apprentissages pour atteindre des objectifs et des tâches spécifiques grâce à une adaptation flexible ». Au final, l’IA se rapportant davantage à un champ technique ou un domaine de recherche qu’aux « systèmes (logiciels ou robots) développés à l’aide de ces techniques », il serait en fait plus exact de parler de « systèmes construits à l’aide de techniques dites d’intelligence artificielle » (Linden et al. 2023).
Bref, il y a presque autant d’acceptions de l'IA que d'auteurs. En retenant que celle-ci désigne « l’ensemble des techniques qui permettent à une machine de simuler l’intelligence humaine, notamment pour apprendre, prédire, prendre des décisions et percevoir le monde environnant » (La Déclaration de Montréal pour un développement responsable de l’intelligence artificielle 2018, p. 19), l'IA enveloppe donc de nombreux sous-domaines, tels que : la vision, la robotique, la planification automatique et l'ordonnancement, le langage naturel, l'apprentissage, la prise de décision, etc. (Rodriguez 2022, p. 46).
2.2.2.	Apprentissage automatique (machine learning)
Souvent associé à l'IA, l'apprentissage automatique (machine learning) n'est qu'une de ses branches dont l'apprentissage repose sur des algorithmes basés sur des statistiques lui permettant d’ « utilise[r] les données et les réponses afin de produire la procédure qui permet d’obtenir les secondes à partir des premières » (Vannieuwenh 2019, p. 28 ; Azencott 2019, p.2 ; Rodriguez 2022, p. 100) le définit comme le « domaine de l'intelligence artificielle qui concerne la conception, l'analyse, le développement et l'implémentation de méthodes et d'algorithmes permettant à une machine d'apprendre, d'évoluer et d'acquérir les connaissances nécessaires afin de réaliser des tâches ou résoudre des problèmes complexes ». On distingue généralement trois types d’apprentissage : supervisé ou non supervisé (i.e. avec données labellisées ou non - pour des tâches de classification, régression et clustering) ainsi que par renforcement (par essais-erreurs, en interaction avec l'environnement afin de maximiser une récompense) (Rodriguez 2022, pp. 102 et 109 ; Charniak 2021, p. 105) (Vannieuwenh 2019, p. 29).
2.2.3.	Apprentissage profond (deep learning)
Au sein de l'apprentissage automatique se distingue à son tour l'apprentissage profond (deep learning), qui repose sur des modèles consistant en plusieurs couches de neurones artificiels bio-inspirés (Vannieuwenh 2019, p. 28) que nous aurons l'occasion d'aborder plus en détails dans une section ultérieure. La frontière entre réseau multicouches « superficiel » (shallow) et réseau profond n'est pas clairement arrêtée et dépend des auteurs (Azencott 2019, p. 104). En bref, ces architectures permettent l'élaboration de modèles paramétriques complexes, à même de capturer des phénomènes non-linéaires, dont les poids (i.e. les paramètres) sont appris via un algorithme à direction de descente (Azencott 2019, p. 104). Elles impliquent l'apprentissage de la représentation de données au niveau des couches intermédiaires (representation learning) (Azencott 2019, p.105). Depuis une vingtaine d'années, ces méthodes ont été catalysées par l'augmentation exponentielle du volume de données digitalisées (64.2 Zettaoctets en 2020, plus de 32 fois plus que dix ans auparavant ; Linden et al. 2023, citant une étude de Statista), le perfectionnement des algorithmes (techniques de régularisation, d'initialisation des poids, etc.), ainsi que par les dispositifs de calcul de plus en plus puissants (GPUs puis TPUs). En parallèle, la complexification des architectures (réseaux convolutifs, récurrents, Transformer, etc.) et des neurones (e.g. LSTM) conjuguée à l'augmentation de leur nombre de couches (profondeur) a concouru à l'accroissement de la performance de ces réseaux de neurones (Sagot 2023a). Ce sont les avancées en vision par ordinateur (alimentées par l'essor des réseaux neuronaux convolutifs incarné par la réussite d'AlexNet lors du défi ImageNet de 2012) qui contribuèrent en premier lieu à la popularité de ces méthodes dans les premières décennies du deuxième millénaire (Zhang et al. 2023). La popularisation de l'apprentissage non-supervisé contribua également à cet essor (Arkhangelskaya et Nikolenko 2023).
2.2.4.	IA générative
Ces dernières années, c'est l'IA dite générative ou GenAI, basée sur de l'apprentissage profond, qui figure en une de tous les médias - au point de réduire parfois toute une discipline à cette seule dimension. Contrairement à l'IA discriminante ou prédictive, l'IA générative permet de générer du contenu (des sorties) nouvelles en se basant sur ses données d'entrainement. Avant d'être consacrée par l'arrivée des modèles transformer pré-entrainés - nous y reviendrons plus loin, les réseaux adversoriels génératifs (GANs) lui avaient déjà assuré une bonne place parmi les utilisations populaires de l'IA (Ali et al. 2024).

{% include links.html %}
