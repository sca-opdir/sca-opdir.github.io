---
title: Architectures neuronales pour le TAL
keywords: IA, LLM, TAL
last_updated: June 18, 2024
tags: [IA, LLM, TAL]
summary: "Evolution des architectures pour le TAL jusqu'aux LLMs"
sidebar: mydoc_sidebar
permalink: LLM_TAL_architectures.html
folder: mydoc
---

repr√©sentation visuelle des Transformers : [https://bbycroft.net/llm](https://bbycroft.net/llm)

tutoriels Transformers [https://github.com/NielsRogge/Transformers-Tutorials](https://github.com/NielsRogge/Transformers-Tutorials) : this repository contains demos made with the Transformers library by ü§ó HuggingFace. Currently, all of them are implemented in PyTorch

1.	Architectures neuronales pour le TAL

1.1.	Principes de fonctionnement des r√©seaux de neurone

La description math√©matique de r√©seaux de neurones auto-organis√©s remonte √† 1948 et Alan Turing d√©j√† (Vannieuwenh 2019, p.24). Le perceptron, premier r√©seau de neurones √† couches, capable d'apprentissage supervis√©, est d√©crit en 1957 par Rosenblatt (Vannieuwenh 2019, pp. 25, 261). Un unique neurone formel n'√©tant capable de s√©parer uniquement des donn√©es lin√©airement s√©parables, dans la pratique plusieurs neurones sont combin√©s en r√©seau multicouches, constitu√©s de nombreuses couches interm√©diaires dites cach√©es (Vannieuwenh 2019, p. 313). En utilisant des fonctions d'activation non lin√©aires (d‚Äôabord sigmo√Ødales ou logistiques, puis d√©sormais fr√©quemment ReLU), un mod√®le param√©trique hautement non lin√©aire peut ainsi √™tre mis au point (Azencott 2019, p. 98). Un r√©seau de neurones est compos√© d‚Äôune s√©quence de fonctions arrang√©es en couches o√π les sorties d'une couche servent d'entr√©e √† la suivante. Ainsi structur√©, le mod√®le peut √™tre complexe et flexible, op√©rant de la sorte comme un approximateur universel.
Nous n'allons pas revenir dans les d√©tails sur le fonctionnement des r√©seaux de neurones. Rappelons ici simplement que l'apprentissage √©mane de l'alternance r√©p√©t√©e, pour chaque observation, d'une phase de propagation avant (somme pond√©r√©e des entr√©es des neurones soumise √† une fonction d'activation afin d'obtenir une valeur de sortie du neurone) et de r√©tropropagation des erreurs (parcours du r√©seau en sens inverse afin d'ajuster les poids des neurones en tenant compte de la diff√©rence entre la valeur attendue et la valeur r√©alis√©e telle que calcul√©e par une fonction de perte) (Vannieuwenh 2019, p. 279 ; Azencott 2019, p. 101). Initialis√©s avec des valeurs le plus souvent al√©atoires, les param√®tres du mod√®le (i.e. les poids des connexions du r√©seau) sont ajust√©s lors de l‚Äôapprentissage en vue de minimiser la fonction d'erreur g√©n√©ralement par descente de gradient, par it√©rations successives jusqu'√† convergence (minimum d'erreur). Le gradient de la fonction d'erreur est d‚Äôabord calcul√© par rapport √† chaque poids, indiquant la direction et l'ampleur de la modification n√©cessaire pour r√©duire l'erreur. Les poids sont ensuite mis √† jour dans la direction oppos√©e au gradient calcul√©, avec une magnitude d√©termin√©e par le taux d'apprentissage. Le gradient correspond √† la pente de la fonction de perte (Charniak 2021, p. 13). Cet algorithme du gradient, qui appartient √† la famille des algorithmes √† directions de descente (Azencott 2019, p. 210), peut √™tre imag√© ainsi : imaginons que l'on souhaite descendre de la montagne en direction de la plaine, le gradient correspondant √† la pente du sol sur lequel nous avan√ßons pas √† pas en direction du bas jusqu'√† attendre la vall√©e. Le "pas" porte √©galement le nom de taux d'apprentissage, un hyperparam√®tre du mod√®le (Vannieuwenh 2019, pp. 113, 279, 282). A c√¥t√© de ce dernier, de nombreux autres choix (initialisation des param√®tres, standardisation des donn√©es, fonctions d'activation, etc.) conditionnent l'atteinte d'un minimum global, qui ne peut par ailleurs jamais √™tre garantie le probl√®me d'optimisation n'√©tant pas convexe (Azencott 2019, pp. 101, 104). En mettant √† profit la r√®gle de d√©rivation en chaine (ou r√®gle de d√©rivation des fonctions compos√©es) et la m√©mo√Øsation, les calculs des gradients permettant l'actualisation des poids sont tr√®s efficaces (Azencott 2019, p. 102), assurant la popularit√© de l'entrainement par r√©tropropagation. Un algorithme de descente de gradient "strict" impliquerait de calculer les modifications des valeurs des param√®tres pour chaque donn√©e d'entrainement avant de proc√©der √† leur mise √† jour. Dans la pratique, cela n'est pas possible en raison de la masse trop volumineuse de donn√©es utilis√©es pour l'entrainement du mod√®le. Pour cette raison, c'est un algorithme de descente de gradient dit stochastique qui est habituellement mis en ≈ìuvre et actualise les valeurs des param√®tres apr√®s avoir trait√© un lot (batch) d'exemples du jeu d'entrainement (Charniak 2021, p. 16).
Si ces principes de base sont partag√©s par la plupart des r√©seaux de neurones, la cr√©ativit√© des sp√©cialistes a engendr√© une grande diversit√© dans la fa√ßon d‚Äôagencer les connexions entre les neurones et la complexification des calculs qui y sont op√©r√©s. Dans les sections qui suivent, nous passons en revue quelques architectures qui marqu√®rent l‚Äô√©volution du deep learning pour le traitement de donn√©es textuelles. Ces √©volutions ont pr√©c√©d√© les Transformers des mod√®les GPT et amen√© progressivement les concepts sur lesquels ces derniers reposent et les briques dont ils se composent. Nous les couvrirons de mani√®re superficielle sans rentrer dans les d√©tails math√©matiques mais en essayant (avec l‚Äôaide de ChatGPT) d'en restituer l'intuition qui les sous-tend.

1.2.	Les r√©seaux √† propagation avant (FFNs) et word embeddings

Dans les r√©seaux √† propagation avant (FFNs), aussi appel√©s perceptrons multicouches (MLPs), l'information circule uniquement dans une direction √† travers plusieurs couches de neurones. Leur architecture simple est √† la base des premiers r√©seaux de neurones utilis√©s dans le TAL. Le travail de Bengio et ses coll√®gues (Bengio et al. 2003) est pr√©curseur des techniques modernes d'apprentissage auto-supervis√©es (√©tiquettes g√©n√©r√©es √† partir des donn√©es elles-m√™mes, se passant d'annotations manuelles externes) et des r√©seaux neuronaux profonds dans le TAL. Le mod√®le de langue est ici formul√© tel un probl√®me de classification (pr√©dire un symbole parmi l'ensemble du vocabulaire √† partir des mots le pr√©c√©dant), que Bengio et al. (2003) proposent de r√©soudre par un r√©seau de propagation avant, qui marque ¬´ le passage de mod√®les discrets √† des repr√©sentations num√©riques ¬ª (Yvon 2022). Leur mod√®le d√©montrait de meilleures performances que les standards des m√©thodes statistiques de l'√©poque, les mod√®les n-gramme (avec g√©n√©ralement n=3, i.e. deux mots de contexte), et apportait une r√©ponse aux probl√®mes de sparsity et storage de ces derniers √©voqu√©s plus haut (Hashimoto 2024a).
Bas√© sur une architecture MLP, le r√©seau de neurones apprend une fonction d'encodage non-lin√©aire qui compresse ¬´ le contexte (les mots pr√©c√©dents) dans un vecteur [dense] de faible dimension, √† partir duquel la distribution conditionnelle des mots successeurs est calcul√©e ¬ª (Yvon 2022). En parall√®le, la projection du vocabulaire dans un espace de dimension r√©duite constitue son plongement lexical. De la sorte, cet article pionnier introduit une architecture neuronale efficace pour des t√¢ches de TAL et de fa√ßon concomitante la notion de repr√©sentation distribu√©e des mots (distributed representation ; en contraste avec les repr√©sentations discr√®tes ou one-hot qui pr√©valaient jusque-l√†), o√π plusieurs dimensions contribuent simultan√©ment √† sa signification. Les vecteurs denses repr√©sentant les mots capturent la similarit√© s√©mantique de ces entit√©s ainsi que des informations syntaxiques dans un espace vectoriel de dimension r√©duite. Ces repr√©sentations vectorielles et leur propri√©t√© permettent une g√©n√©ralisation du mod√®le plus efficace que celle bas√©e sur la co-occurence de mots dans le corpus des mod√®les n-gramme et apportent ainsi une r√©ponse au probl√®me de la mal√©diction de la dimensionnalit√© (curse of dimensionality) (Bengio et al. 2003). L'architecture bipartite propos√©e par les auteurs prend en entr√©e une fen√™tre de texte de taille fixe, consid√©rant chaque segment textuel de fa√ßon ind√©pendante. Il inclut ensuite une couche cach√©e qui apprend les repr√©sentations vectorielles distribu√©es des mots et une autre partie mod√©lisant la probabilit√© des s√©quences de mots. Les repr√©sentations vectorielles distribu√©es des mots sont apprises par le mod√®le en m√™me temps que la fonction de probabilit√© pour les s√©quences de mots (Bengio et al. 2003).
Bengio et al. (2003) ont pos√© les concepts fondamentaux pour l'utilisation des r√©seaux de neurones dans le TAL et l'apprentissage des repr√©sentations vectorielles des mots en fonction du contexte. Il fallut toutefois attendre des d√©veloppements ult√©rieurs pour mettre au point des m√©thodes neuronales d'embedding efficaces et utilisables √† grande √©chelle (ChatGPT, discussion du 13.01.2024). Word2vec (Mikolov et al. 2013) est l'une d'elles. D√©velopp√©e une d√©cennie plus tard, elle a √©t√© sp√©cifiquement con√ßue pour un apprentissage auto-supervis√© et rapide des plongements de mots √† partir de grands corpus textuels. A la diff√©rence du mod√®le de Bengio et al. (2003), qui apprenait les plongements dans le cadre de la t√¢che de pr√©diction du mot suivant, l'optimisation r√©alis√©e par Word2vec se focalise directement sur la similarit√© s√©mantique. D'architecture de type FFN peu profonde, Word2vec se d√©cline en deux mod√®les : Skip-Gram, qui pr√©dit les mots contextuels √† partir d'un mot cible, et CBOW (Continuous Bag of Words), qui pr√©dit un mot cible √† partir de mots contextuels. Par opposition aux m√©thodes pr√©c√©dentes qui utilisaient des informations statistiques globales de co-occurrence, ces mod√®les sont dits shallow window-based, car ils ¬´ learn word embeddings by making predictions in local context windows ¬ª (Mundra et al. 2019). Dans les deux cas, l'apprentissage repose sur la projection lin√©aire des embeddings de mots (sans couche cach√©e √† activation non-lin√©aire). Cette t√¢che de vectorisation peut √™tre √©tendue aux documents (Doc2Vec ; Charniak 2021, p. 86). Bien que Word2vec ne soit pas un mod√®le de langue au sens strict (pr√©diction s√©quentielle des mots), il contribua grandement aux am√©liorations des t√¢ches de TAL en fournissant une repr√©sentation num√©rique vectorielle des mots comme entr√©es aux mod√®les de langage neuronaux.
Si le r√©seau pr√©curseur du laboratoire de Bengio (Bengio et al. 2003) posa les bases des mod√®les de langue √† architecture neuronale profonde, il souffrait n√©anmoins de plusieurs limitations inh√©rentes aux r√©seaux √† propagation avant. Parmi les principales, les FFNs sont incapables de g√©rer des s√©quences de taille variable et, au-del√† d'une fen√™tre de contexte limit√©e, ne captent pas les d√©pendances longue distance. Aussi, la taille du mod√®le augmente avec la taille de la fen√™tre et les entr√©es ne sont pas trait√©es de fa√ßon sym√©trique (poids diff√©rents √† chaque entr√©e) (Hashimoto 2024a). Les m√©thodes comme Word2vec, bien qu'elles apportent une avanc√©e significative pour la g√©n√©ration d'embeddings, n'adressent pas ces limitations. 
Etant donn√© leurs succ√®s dans la vision par ordinateur dans les premi√®res d√©cennies du deuxi√®me mill√©naire, il n‚Äôest pas surprenant que les r√©seaux de neurones convolutifs (CNNs) aient √©t√© adopt√©s par les sp√©cialistes du traitement du langage (Chaubard et Socher 2019). Ils obtinrent de bonnes performances, notamment pour des t√¢ches de classification de phrases (Young et al. 2018). Toutefois, contenant un grand nombre de param√®tres entrainables, ils requi√®rent une quantit√© importante de donn√©es d‚Äôentrainement (Young et al. 2018). De plus, ils sont difficiles √† entrainer, moins stables et moins flexibles que les r√©seaux de neurones r√©currents (RNNs) (Yin et al. 2017 ; Young et al. 2018). Bien que les CNNs aient pu atteindre des performances comp√©titives dans certaines t√¢ches (Yin et al. 2017 ; Young et al. 2018), leur importance semble donc moindre dans l‚Äô√©volution des architectures neuronales d√©di√©es au TAL. Pour cette raison, nous ne les d√©taillerons pas davantage (pas plus que les mod√®les hybrides CNNs-RNNs) et nous focalisons directement les RNNs.

1.3.	R√©seaux de neurones r√©currents (RNNs)

Les r√©seaux de neurones r√©currents (RNNs) sont des r√©seaux dont le graphe de connexion contient au moins 1 cycle. Alors que les r√©seaux de neurones √† propagation avant tels que propos√©s par Bengio et al. (2003), tout comme les mod√®les n-gramme, tiennent compte d'un contexte (mots voisins) de taille limit√©e, les RNNs l√®vent cette contrainte et lib√®re de l'hypoth√®se des d√©pendances locales (Yvon 2022). La pr√©diction du mot suivant se fait de proche en proche en s'appuyant sur un √©tat interm√©diaire cach√©. Par ailleurs, puisqu'ils traitent s√©quentiellement un √©l√©ment apr√®s l'autre ‚Äì chacun √©tant associ√© √† un vecteur num√©rique de m√™me taille ‚Äì ils peuvent prendre en entr√©e des s√©quences de longueur variable, l√† o√π un perceptron n√©cessiterait que toute la s√©quence lui soit pass√©e en une fois. Dans cette architecture, les poids des connexions entre neurones (param√®tres du mod√®le) sont donc partag√©s (r√©utilis√©s) √† chaque √©tape temporelle. Le partage des param√®tres assure que leur nombre reste r√©duit et constant quelle que soit la longueur de la s√©quence, et am√©liore la capacit√© de g√©n√©ralisation et d‚Äôapprentissage des d√©pendances √† longue distance en traitant les entr√©es de mani√®re sym√©trique (Mohammadi et al. 2019 ; Gallego et R√¨os Insua 2022). Cette architecture cyclique convient dans les situations o√π des donn√©es ant√©rieures influent sur les donn√©es suivantes de fa√ßon relativement lointaine telles que l'analyse de s√©quences ou de s√©ries temporelles ou spatiales. Cette configuration est donc particuli√®rement ad√©quate pour des t√¢ches de mod√©lisation du langage.
A noter que dans leur conception les RNNs se rapprochent de mod√®les non-neuronaux d√©j√† utilis√©s pour le TAL. Par exemple, ils partagent plusieurs similarit√©s avec les mod√®les de Markov cach√©s (HMM). Tous deux sont des mod√®les de graphes dirig√©s, bas√©s sur des √©tats cach√©s pour d√©terminer des probabilit√©s d'√©missions (ChatGPT, discussion du 13.01.24). Ils se diff√©rencient toutefois puisque l'√©tat cach√© d'un RNN, calcul√© sur la base de l'√©tat cach√© pr√©c√©dent et de l'observation actuelle, est d√©termin√© selon une fonction d√©terministe. Dans un HMM, c'est une distribution de probabilit√©s qui sert √† d√©terminer (al√©atoirement donc) l'√©tat cach√© actuel en se basant sur les probabilit√©s de transition (param√®tres du mod√®le) et sans d√©pendre directement de l'observation actuelle (celle-ci influe indirectement, par le biais des probabilit√©s d'√©mission - autres param√®tres du HMM). Les HMMs ne capturent pas les d√©pendances longue distance bidirectionnelles et souffrent de la mal√©diction de la dimensionalit√© (curse of dimensionality) puisque le nombre d‚Äô√©tats possibles augmente exponentiellement avec la taille du contexte (Dey 2023).
En fonction du type de transformations d√©sir√©es, les RNNs peuvent rev√™tir diverses configurations, en many-to-one (e.g. classification de texte), one-to-many (e.g. g√©n√©ration de texte) ou many-to-many (e.g. traduction automatique), ce qui leur conf√®re une aptitude √† accomplir une grande diversit√© de t√¢ches. Mais tous ont en commun qu‚Äô√† chaque pas de la s√©quence, un √©tat cach√©, conditionn√© par les √©tats cach√©s pr√©c√©dents, est mis √† jour et transmis d'une position √† l'autre. Cette mise √† jour peut √™tre effectu√©e selon diff√©rentes modalit√©s. Quelques-unes d‚Äôentre elles sont exemplifi√©es ci-apr√®s.
1.3.1.	RNNs simples (vanilla RNNs ou RNNs de Elman)
Par contraste avec les r√©seaux de propagation avant (graphe acyclique orient√©), les RNNs sont caract√©ris√©s par une structure o√π les sorties reviennent au point d'entr√©e dans le r√©seau (graphe cyclique orient√©) (Charniak 2021, p. 76). Ainsi, au d√©but de la phase de propagation la sortie du r√©seau de la pr√©c√©dente it√©ration est concat√©n√©e avec le plongement courant. Cela leur donne la capacit√© de garder une "m√©moire" des entr√©es pr√©c√©dentes √† travers leur √©tat cach√©. Pour les mod√®les de langue, ce vecteur cach√© est compar√© √† une banque d'embeddings par un produit scalaire, suivi d'une fonction softmax transformant les scores obtenus en une distribution de probabilit√©s sur tous les mots du lexique en vue de choisir le mot le plus probable (Sagot 2023d).
Pour le reste, le calcul de la passe avant ne diff√®re pas consid√©rablement des r√©seaux √† propagation avant, appliquant des fonctions d'activation non-lin√©aires comme tanh ou ReLU pour calculer l'√©tat cach√© √† chaque pas de temps. Pour la phase de r√©tropropagation, l'ajustement des param√®tres ne peut manifestement pas √™tre conduit √† l'identique : dans les RNNs, il faudrait remonter l'erreur pour toutes les boucles de r√©currence et ajuster les param√®tres ¬´ gr√¢ce aux contributions de tous les mauvais choix depuis le premier mot ¬ª (Charniak 2021, p. 77). Dans la pratique, le nombre d'it√©rations ant√©rieures prises en compte pour la mise √† jour des param√®tres est d√©fini arbitrairement via un hyperparam√®tre, la taille de fen√™tre. Les phrases sont d√©limit√©es par le mot STOP afin que les limites de la fen√™tre ne soient pas tenues de co√Øncider avec celles de la phrase. Ce type de r√©tropropagation est appel√© r√©tropropagation √† travers le temps. Comme d√©j√† mentionn√©, la projection de la repr√©sentation interne donn√©e par une fonction d'encodage non-lin√©aire du contexte, calcul√©e de fa√ßon r√©cursive, permet d'obtenir la distribution conditionnelle de sortie qui lui est associ√©e, utilis√©e dans des applications telles que la reconnaissance de la parole, la g√©n√©ration de texte et la traduction automatique (Yvon 2022). L'√©tat initial du r√©seau avant entrainement est souvent mis √† z√©ro, mais s‚Äôactualise √† chaque √©tape pour int√©grer l'information des entr√©es pr√©c√©dentes, refl√©tant ainsi le flux continu d'information dans les s√©quences de donn√©es.
Tout comme les FNNs, les RNNs peuvent √™tre multicouches (multi-layer ou stacked RNNs) : les repr√©sentations produites par une couche de neurones constituent des repr√©sentations interm√©diaires, donn√©es en entr√©e de la couche suivante (Mohammadi et al. 2019). En augmentant leur profondeur, des representations plus complexes peuvent √™tre apprises : ¬´ lower RNNs should compute lower-level features and the higher RNNs should compute higher-level features ¬ª (Hashimoto 2024b). Compar√©e √† celle des CNNs ou FFNs, la profondeur des RNNs les plus performants est tr√®s modeste car elle se limite g√©n√©ralement √† 4 couches tant pour l‚Äôencodeur que le d√©codeur (Hashimoto 2024b).
La structure des RNNs basiques implique que l'influence que peut avoir un mot sur un autre diminue avec l'√©loignement, ce qui rend l'ajustement des poids par calcul de gradient (i.e. l'apprentissage) plus compliqu√© (instabilit√©s num√©riques ; Yvon 2022). Pour pallier ces probl√®mes de vanishing ou exploding gradient et permettre de mieux capter les d√©pendances longue distance, des adaptations √† l'architecture des RNNs ont √©t√© d√©velopp√©es. Celles-ci font recours √† un m√©canisme de portes qui assiste la r√©gulation du flux d'information le long de la s√©quence et aide √† se focaliser sur l'information importante (Kaylan 2023).
1.3.2.	RRNs √† m√©moire augment√©e - LSTMs et GRUs
Elabor√©s dans une forme primitive en 1997 d√©j√† (Hochreiter and Schmidhuber 1997), les r√©seaux de neurones de longue m√©moire √† court terme (LSTM) ont notamment √©t√© d√©velopp√©s pour pallier les lacunes des RNNs dans leur faible capacit√© de m√©morisation longue distance. Afin d'am√©liorer cette derni√®re, ¬´ les LSTM transmettent deux versions du pass√© : la m√©moire s√©lective "officielle" [dont la chronologie est appel√©e "√©tat de cellule" - cell state] et une version plus locale ¬ª (Charniak 2021, p. 83). La premi√®re, un vecteur mis √† jour √† chaque √©tape, v√©hicule les informations √† longue distance et repr√©sente donc la m√©moire √† long terme. La deuxi√®me, l‚Äô√©tat cach√©, est constitu√©e de l‚Äôinformation r√©cente (pr√©diction pr√©c√©dente et entr√©e courante), formant ainsi la m√©moire √† court terme. Autrement dit, une cellule LSTM g√®re une m√©moire √† court terme tout en maintenant une m√©moire √† long terme. Et ce via trois portes : d'entr√©e (input gate; qui s√©lectionne l'information de l‚Äô√©l√©ment courant √† rajouter √† la m√©moire interne), d‚Äôoubli (forget gate ; qui d√©termine les informations √† retirer (i.e. mettre √† z√©ro) de la m√©moire interne) et de sortie (output/exposure gate ; qui g√©n√®re la sortie en prenant en compte l‚Äô√©l√©ment courant et la m√©moire interne, elle s√©pare ¬´ the final memory from the hidden state ¬ª ce qui actualise la m√©moire √† court terme) (Mohammadi et al. 2019). Ce m√©canisme doit permettre d'√©liminer s√©lectivement de la m√©moire les √©v√©nements du pass√© peu importants √† chaque pas de temps. Il en ressort un √©tat cach√© ainsi qu'un vecteur de contexte, l‚Äô√©tat de cellule (ce que la cellule a gard√© en m√©moire).
Par ce m√©canisme de portes, le LSTM m√©lange les diff√©rentes m√©moires et les entr√©es. Le r√©sultat de l'it√©ration pr√©c√©dente est concat√©n√© au plongement de l'unit√© lexicale courante avant de passer par une premi√®re √©tape produisant un ¬´ signal oublieux ¬ª (multiplication terme √† terme de la m√©moire avec la sortie d'une sigmo√Øde - born√©e entre 0 et 1 ; un exemple de filtrage logiciel ou soft gating). Dans la partie additive, le plongement de mot traverse deux couches lin√©aires, l'une √† fonction d'activation sigmo√Øde, l'autre tanh. Cette derni√®re, prenant des valeurs entre -1 et 1, permet l'extraction de nouvelles informations (Charniak 2021, p. 85). Les r√©sultats de ces deux op√©rations sont multipli√©s terme √† terme et ajout√©s √† l'√©tat de la cellule. La trajectoire de la ligne m√©moire se scinde : une des branches passe par une fonction tanh avant d'√™tre ¬´ combin√©[e] avec une transformation lin√©aire de l'historique/plongement le plus local ¬ª (Charniak 2021, p. 85), et finalement concat√©n√©e avec le plongement du mot suivant √† l'it√©ration suivante. Ainsi, durant tout le processus ¬´ la ligne de m√©moire de cellule ne passe jamais directement au travers d‚Äôunit√©s lin√©aires ¬ª (Charniak 2021, p. 85). En r√©sum√©, le fonctionnement des LSTMs repose sur deux couches cach√©es, l'une contr√¥lant ce qui est retir√© de la ligne de m√©moire par multiplication d'un vecteur de nombres compris entre 0 et 1 (oubli), l'autre ce qui lui est ajout√© par addition d'un vecteur de m√™me type ; ainsi que sur trois portes (d‚Äôentr√©e, d‚Äôoubli, de sortie) qui y r√©gulent le flux d‚Äôinformation. Les LSTMs ont repr√©sent√© l‚Äô√©tat de l'art du TAL avant l'arriv√©e des Transformer (Sagot 2023d). Leur architecture est toutefois d'une complexit√© certaine et ils sont gourmands en m√©moire (Zhang et al. 2023). Ils ne r√©solvent par ailleurs pas totalement le probl√®me de circulation de l‚Äôinformation (explosion ou disparition du gradient) et ne permettent pas un passage √† l‚Äô√©chelle en raison d‚Äôun entrainement difficilement parall√©lisable en raison de la s√©quentialit√© des op√©rations et leur profondeur d√©passe ainsi rarement 8 couches (Sagot 2023d).
Avant de conclure cette partie sur les RNNs, mentionnons encore les r√©seaux √† portes r√©currents (GRU), une autre version "am√©lior√©e" des RNNs, qui, contrairement aux LSTMs, se base sur une seule ¬´ ligne de m√©moire ¬ª (Charniak 2021, p. 90). Se passant d‚Äôune output gate explicite, les GRUs sont constitu√©s de deux portes plut√¥t que trois (Mohammadi et al. 2019). L'une dite de r√©initialisation (reset gate) se charge de d√©terminer si l‚Äôinformation pass√©e doit √™tre oubli√©e, l'autre dite de mise √† jour (update gate) est responsable des informations qui seront transmises plus loin (quelle quantit√© d‚Äôinformation pass√©e conserver, quelle quantit√© d‚Äôinformation nouvelle ajouter). Avec une seule porte de mise √† jour, le mod√®le ne diff√©rencie pas l‚Äô√©tat de la cellule de l‚Äô√©tat cach√© et expose l‚Äôenti√®ret√© de l‚Äô√©tat cach√© puisqu‚Äôaucune porte ne lui sp√©cifiquement d√©di√© (Arkhangelskaya et Nikolenko 2023). Ceci implique un nombre moindre de param√®tres et, partant, un entrainement facilit√©. Bien que n√©cessitant un entrainement plus long, LSTMs et GRUs (entre autres variantes de RNNs) induisent une am√©lioration de la performance des mod√®les neuronaux r√©currents (Kaylan 2023). 
En r√©sum√©, les RNNs permettent de tenir compte des informations ant√©rieures de la s√©quence th√©oriquement de mani√®re illimit√©e (Mohammadi et al. 2019) et, dans la pratique, de mod√©liser les d√©pendances entre les √©l√©ments jusqu‚Äô√† une certaine distance (200 tokens selon Cao et al. 2023). Avec des s√©quences d‚Äôentr√©e pouvant √™tre de taille variable, ceci repr√©sente des am√©liorations majeures par rapport aux r√©seaux √† propagation avant ou aux mod√®les n-grammes. Aussi, la taille du mod√®le n‚Äôaugmente pas avec la longueur de la s√©quence (Mohammadi et al. 2019). Toutefois, ils sont difficiles √† entrainer et affichent de mauvaises performances pour capturer les interd√©pendances entre √©l√©ments √©loign√©s (Mohammadi et al. 2019). Des variantes (LSTMs, GRUs, etc.) apportent une r√©ponse (limit√©e) √† ce probl√®me, elles sont n√©anmoins co√ªteuses car font appel √† davantage de param√®tres et √† des structures internes plus compliqu√©es. Et si la sophistication des mod√®les r√©currents a permis l'am√©lioration de la capacit√© de m√©moire du r√©seau de neurones, elle n'abolit pas une autre contrainte : une s√©quence doit traverser le r√©seau mot apr√®s mot, aussi bien pour l'entrainement qu'en phase de pr√©diction, et le calcul de la passe courante repose sur les valeurs calcul√©es √† la passe pr√©c√©dente (mod√®le autor√©gressif ; Yvon 2022). Ceci rend impossible toute parall√©lisation - particuli√®rement pr√©cieuse en phase d'apprentissage.

C'est sur la base d‚Äôun LSTM que furent propos√©s pour la premi√®re fois des mod√®les dits s√©quence-√†-s√©quence (seq2seq) par Google en 2014 (Sutskever et al. 2014). Deux RNNs sont mis en action : la repr√©sentation latente apprise par le premier (encodeur) sert d'entr√©es √† un deuxi√®me qui g√©n√®re la s√©quence (d√©codeur). Particuli√®rement utilis√© pour la traduction automatique, cet apprentissage s√©quence-√†-s√©quence (many-to-many) met en correspondance une s√©quence de symboles avec une autre s√©quence de symboles, plut√¥t que d'associer individuellement les symboles (Charniak 2021, p. 89). Diverses t√¢ches de TAL peuvent √™tre formul√©es comme une mod√©lisation seq2seq : r√©sum√© de textes, dialogues, g√©n√©ration de code, etc.. (Hashimoto 2024b). A la diff√©rence de r√©seaux de neurones de type perceptrons multicouches, il n'est plus requis que la s√©quence de sortie (qui peut √™tre de longueur variable) produite soit de m√™me longueur que la s√©quence d'entr√©e (de longueur variable √©galement) (Kaylan 2023). L‚Äôentr√©e comme la sortie peut donc √™tre de longueur arbitraire, bien qu‚Äôil soit connu que la performance d√©cline pour de tr√®s longs inputs en raison des limitations des RNNs (Genthial et al. 2019). Une autre avanc√©e permise par les mod√®les seq2seq ‚Äúespecially with its use of LSTMs, is that modern translation systems can generate arbitrary output sequences after seeing the entire input‚Äù (Genthial et al. 2019). Dans un mod√®le de ce type, un premier r√©seau (compos√© de LSTMs ou autres variations de RNNs), l'encodeur, produit un vecteur de taille fixe qui repr√©sente un plongement de phrase contextualis√© (phase d'encodage). L'√©tat final de l'encodeur (ou la somme (ou autre agr√©gation) de tous les √©tats selon certaines variantes ; Charniak 2021, p. 95) est transmis comme point d'entr√©e au d√©codeur. Dans cette deuxi√®me phase, dite de d√©codage, le but est de pr√©dire le mot suivant dans le langage cible. La couche finale retourne donc un vecteur donnant la probabilit√© d'occurrence √† la position correspondante de chaque item du vocabulaire cible. Des corpus parall√®les sont donc n√©cessaires pour la phase d'apprentissage, qui se r√©alise comme d'accoutum√©e par optimisation de la fonction de perte (g√©n√©ralement l'entropie crois√©e) (Charniak 2021, p. 91). Lors de la phase d'inf√©rence, la sortie de l‚Äôunit√© pr√©c√©dente est utilis√©e comme entr√©e dans l‚Äôunit√© suivante et la g√©n√©ration de texte se termine lorsque le d√©codeur pr√©dit le token de fin de s√©quence. En plus de leur application initiale pour la traduction automatique (neural machine translation), les mod√®les seq2seq ont √©t√© abondamment utilis√©s pour d'autres t√¢ches de TAL, notamment comme chatbots (jusqu'√† l'arriv√©e des Transformers).
En traitant les s√©quences en respectant leur ordonnancement temporel (i.e. l'ordre des mots dans la phrase), un RNN simple n'a pas acc√®s √† l'information future (i.e. les mots √† droite du mot cible). Cette restriction est lev√©e avec les RNNs bi-directionnels, une disposition dans laquelle deux mod√®les ind√©pendants parcourent les s√©quences de texte dans les deux sens (de gauche √† droite et inversement) et dont les sorties sont concat√©n√©es apr√®s la couche de sortie. Puisqu‚Äôil maintient ainsi deux couches cach√©es (une pour la propagation de gauche √† droite, une pour celle de droite √† gauche), un tel r√©seau requiert toutefois le double de place en m√©moire pour les param√®tres de poids et de biais (Mohammadi et al. 2019). Requ√©rant une s√©quence d‚Äôentr√©e dans son entier, les RNNs bi-directionnels ne sont pas utilisables comme mod√®les de langue o√π seul le contexte de gauche est disponible (Hashimoto 2024b). En particulier, les LSTM bi-directionnels (BiLSTMs) ont √©t√© utilis√©s avec succ√®s √† des fins de pur encodage (g√©n√©ration de plongements lexicaux enrichis). Cette t√¢che, ainsi affranchie du besoin de corpus annot√©s, peut donc s'appuyer sur une quantit√© faramineuse de donn√©es. Ces repr√©sentations latentes contextuelles sont utilis√©es ensuite comme entr√©es pour d'autres t√¢ches de TAL, pour lesquelles les corpus disponibles sont potentiellement plus limit√©s. Le BiLSTM ELMo (Peters et al. 2018) a notamment marqu√© une √©tape importante dans l'√©volution des grands mod√®les de langue en catalysant l'adoption de deux concepts qui domineront les mod√®les ult√©rieurs (Guimar√£es et al. 2024). Tout d'abord, l'apprentissage d'embeddings contextuels profonds l√† o√π Word2vec ou GloVe ne fournissaient que des repr√©sentations statiques (un mot associ√© √† une repr√©sentation vectorielle unique, ind√©pendamment du contexte d'utilisation). Prenant en entr√©e des vecteurs qui encodent la s√©quence de caract√®res de chacun des tokens, il se passe en outre d'un vocabulaire pr√©d√©termin√© de m√™me d'un d√©coupage en sous-mots, ce qui est particuli√®rement ad√©quat pour traiter les mots rares ou absents du corpus d'apprentissage (Sagot 2023d). Le d√©roulement de l'apprentissage ensuite : un pr√©-entrainement sur un corpus massif de donn√©es non √©tiquet√©es, suivi d'un affinement sur des t√¢ches sp√©cifiques avec un corpus plus restreint sans mise √† jour des param√®tres du mod√®le pr√©-entrain√©, qui sont dits "gel√©s" (Yvon 2022). 
Un autre concept d√©velopp√© √† l‚Äô√®re des RNNs est celui de l‚Äôattention. Celle-ci fut mise en avant par Bahdanau en 2014 (Bahdanau et al. 2015) √† des fins de traduction automatique neuronale avec un RNN s√©quence-√†-s√©quence constitu√© d‚Äôunit√©s similaires aux LSTMs. Dans ce contexte, l‚Äôattention permet au d√©codeur de ¬´ regarder ¬ª de fa√ßon pond√©r√©e les vecteurs produits par l‚Äôencodeur, r√©sultant dans une sorte d‚Äôalignement entre la phrase d‚Äôentr√©e et celle de sortie (facilitant l‚Äôappariement entre un mot fran√ßais et son √©quivalent en anglais par exemple). Divers autres mod√®les d‚Äôattention ont par la suite √©t√© pr√©sent√©s (Genthial et al. 2019). Entre autres fut √©tabli peu de temps apr√®s le principe d‚Äôintra-attention (popularis√© plus tard sous le terme d‚Äôauto-attention ; Cheng et al. 2016) pour mod√©liser les d√©pendances √† l‚Äôint√©rieur m√™me de l‚Äôencodeur et du d√©codeur.
Plusieurs concepts initi√©s ou approfondis avec les RNNs (la configuration encodeur-d√©codeur, le pr√©-entrainement, les plongements contextuels et l‚Äôattention) serviront de pierres angulaires aux architectures ult√©rieures pour faire progresser le traitement automatique neuronal des langues.

1.4.	Transformer

A l‚Äôheure o√π les mod√®les g√©n√©ratifs de type VAE ou GAN atteignaient des performances remarquables pour la synth√®se d‚Äôimages (Wang et al. 2020) et connaissaient √©galement des applications pour la g√©n√©ration de texte (Dasgupta et al. 2023), l'arriv√©e des Transformers en 2017 (Vaswani et al. 2017) marqua un v√©ritable tournant dans le domaine de l‚Äôapprentissage profond. Introduit √† des fins de traduction automatique, il existe d√©sormais des implantations efficaces des Transformers appliqu√©es √† des t√¢ches et modalit√©s les plus diverses (traitement de sons, images, etc.). Nous nous contenterons ici √† leur utilisation sur des donn√©es textuelles.
Le but principal du Transformer consiste √† repr√©senter des donn√©es s√©quentielles en tenant compte de leur contexte, comme dans un RNN, mais sans que cela ne d√©pende d'un m√©canisme de r√©currence (i.e. la repr√©sentation du mot pr√©c√©dent n'est pas requise pour cette op√©ration). En l‚Äôabsence d'empilement temporel des √©tats cach√©s, les calculs peuvent √™tre parall√©lis√©s et, partant, une r√©duction des temps d'entrainement et des performances am√©lior√©es. Par ailleurs, alors que dans les RNN l'importance que les mots voisins peuvent se voir attribuer diminue avec la distance, les Transformers exploitent le m√©canisme d‚Äôattention (d√©crit ci-apr√®s) pour lever cette limitation. Gr√¢ce √† celui-ci, ils sont en effet capables de consid√©rer de fa√ßon √©galitaire tous les √©l√©ments constitutifs de la s√©quence : la repr√©sentation d'un mot √† une position donn√©e de la s√©quence d‚Äôentr√©e prend en compte les repr√©sentations de toutes les positions de la s√©quence, captant ainsi le contexte global. 
Forts de ces deux innovations cl√©s, les Transformers ont rapidement supplant√© les RNNs pour de nombreuses t√¢ches de TAL (Raiaan et al. 2024). Yvon (2022) en identifie principalement trois : la fouille de texte et de recherche d'information, l'analyse linguistique, et la g√©n√©ration de texte. Chacune d'elles tirent profit de diff√©rentes capacit√©s de ce type de r√©seau de neurones - respectivement, ¬´ la richesse des repr√©sentations internes [, leur capacit√©] √† prendre en compte des d√©pendances √† tr√®s longue distance [, ] leur capacit√© pr√©dictive ¬ª. L'auteur n'h√©site d‚Äôailleurs pas √† d√©peindre le Transformer comme le "couteau suisse" des linguistes. D'autant plus que s‚Äôajoutent √† cela d‚Äôautres avantages que nous d√©couvrirons au fil des paragraphes qui suivent et d√©crivent l'un apr√®s l'autre les principaux composants du Transformer s√©minal (Vaswani et al. 2017). 

1.4.1.	Vecteurs d‚Äôentr√©e
L'encodeur comme le d√©codeur attendent en entr√©e les repr√©sentations vectorielles des mots. Ainsi de nombreuses √©tapes de pr√©paration de texte (nettoyage, filtrage, etc. ‚Äì non pass√©es en revue ici) sont n√©cessaires √† la conversion des donn√©es textuelles en vecteurs d'entr√©e pour les mod√®les. Bien que nous saisissions l‚Äôopportunit√© de le mentionner ici, ceci n‚Äôest pas propre au mod√®le Transformer.
Une √©tape importante de ce ¬´ pr√©-traitement ¬ª est la tokenisation. Selon la m√©thodologie utilis√©e, un ¬´ token ¬ª peut √™tre un mot, un sous-mot, ou un caract√®re. Par exemple, les mod√®les GPT-2, RoBERTa ou BART utilisent l'encodage byte-pair (Byte Pair Encoding ou BPE ; Hugging Face 2022). Propos√© en 1994 comme m√©thode de compression de donn√©es, elle proc√®de ainsi : en commen√ßant par un ensemble de symboles de base (par exemple, l'alphabet), les paires de tokens cons√©cutifs fr√©quents dans le corpus sont combin√©es (fusionn√©es) de mani√®re it√©rative pour former de nouveaux tokens. Pour chaque fusion, le crit√®re de s√©lection est bas√© sur la fr√©quence de co-occurrence de deux tokens contigus : la paire la plus fr√©quente est s√©lectionn√©e. Le processus de fusion continue jusqu'√† atteindre la taille pr√©d√©finie. Pour sa part, BERT utilise l'algorithme de tokenisation WordPiece et ajoute deux tokens sp√©ciaux ([CLS] et [SEP] en d√©but et fin de phrase). WordPiece est un algorithme mis au point par Google pour des syst√®mes de recherche vocale (Khanna 2021). ChatGPT nous explique que cet algorithme diff√®re de BPE car il vise √† optimiser "un crit√®re l√©g√®rement diff√©rent, en se concentrant sur la minimisation de la probabilit√© de la perte sur l'ensemble de donn√©es lors de la formation des tokens. Cela permet √† WordPiece de mieux g√©rer les mots rares ou inconnus en les d√©composant en sous-unit√©s plus significatives. De plus, WordPiece commence avec un vocabulaire de base plus riche, incluant non seulement des caract√®res individuels mais aussi des sous-cha√Ænes de caract√®res fr√©quemment observ√©es, ce qui facilite une tokenisation plus efficace et granulaire" (ChatGPT, GPT-4 ; conversation du 20.02.2024).

1.4.2.	Encodage positionnel (positional encoding)
Si l'absence de s√©quentialit√© dans le traitement du texte par le Transformer permet son traitement en parall√®le, ceci entraine √©galement une perte d'information sur leur position dans la s√©quence. Pour pallier √† cet effet ind√©sirable, plusieurs techniques d'encodage positionnel (soit appris soit statique s‚Äôil est calcul√© par une fonction d√©terministe) ont √©t√© mises au point afin de pouvoir int√©grer (g√©n√©ralement par simple addition de vecteurs) les indices positionnels aux vecteurs d'entr√©e (Yvon 2022). Dans leur l'√©tude, Vaswani et son √©quipe ont obtenu des r√©sultats similaires que l'encodage soit bas√© sur des fonctions trigonom√©triques ou appris durant l'entrainement (Vaswani et al. 2017). Apr√®s l'encodage positionnel, le vecteur contient donc pour chaque token une information sur sa signification et sur sa position dans la s√©quence qui aide le mod√®le √† √™tre plus pr√©cis grammaticalement et s√©mantiquement (Raiaan et al. 2024). Une autre alternative consiste √† inclure dans le calcul de l'attention un terme int√©grant la notion de position relative pour tenir compte de l'√©loignement entre deux mots (Press et al. 2022).
1.4.3.	L'attention
L'id√©e de base derri√®re les m√©canismes d'attention r√©side dans l'hypoth√®se que certains segments de la s√©quence source sont plus informatifs que d'autres pour pr√©dire la s√©quence cible. L'impl√©mentation propos√©e par Vaswani et ses coll√®gues (Vaswani et al. 2017) dite scaled dot-product attention, diff√®re de celle pr√©c√©demment usit√©e avec les mod√®les RNN seq2seq. Plut√¥t que de s'appuyer sur des √©tats cach√©s s√©quentiels pour transmettre l'information √† travers le temps, cette m√©thode utilise des matrices pour repr√©senter les requ√™tes, les cl√©s et les valeurs. Elle partage certains traits communs avec la recherche d'information dans une base de donn√©es (Koubaa et al. 2023 ; Poupart 2019) : le m√©canisme d'attention d√©termine l'importance relative de chaque valeur en fonction de la correspondance entre la requ√™te et les cl√©s. Sous cet angle, une base de donn√©es contient un ensemble de Valeurs (par exemple des articles de journaux) auxquelles sont associ√©es des Cl√©s (par exemple des mots-cl√©s r√©sumant chacun des articles). Dans cette analogie, lorsqu'une Requ√™te (recherche d'un article de journal) est soumise, le moteur de recherche compare la Requ√™te aux Cl√©s (gr√¢ce √† une fonction de similarit√©) et retourne la Valeur (l'article de journal pr√©sent dans la base de donn√©es) associ√©e √† la plus forte similarit√©. Pour s√©lectionner une unique valeur de retour, la fonction de similarit√© produit dans cette exemple un vecteur one-hot" contenant un 1 pour l'article maximisant le crit√®re de recherche, autrement 0. 
Dans le cas du Transformer, nous cherchons √† confronter un mot (la Requ√™te) √† tous les autres mots de la s√©quence (les Cl√©s). La fonction de similarit√© doit permettre d'√©valuer l'intensit√© de l'attention que le mot doit porter aux autres mots de la s√©quence. Plut√¥t que de retourner uniquement le mot qui maximise cette similarit√©, une pond√©ration est ici effectu√©e : la fonction de similarit√© produit des scores d'attention qui ne sont pas des vecteurs one-hot mais des valeurs comprises entre 0 et 1 qui permettent d'accentuer les informations les plus pertinentes par multiplication avec les Valeurs (ce qui est retourn√© de la base de donn√©es) correspondant aux autres mots de la s√©quence (comme si le moteur de recherche renvoie une liste d'articles pond√©r√©e par leur pertinence). 
Ces diff√©rentes √©tapes se produisent simultan√©ment pour tous les mots de la s√©quence par calcul matriciel. Avant le calcul de l'attention, des projections lin√©aires des embeddings des mots sont calcul√©es en multipliant ces derniers √† 3 matrices de poids (param√®tres du mod√®le appris durant l'entrainement) de mani√®re √† g√©n√©rer les matrices de Requ√™tes (Q), Cl√©s (K) et Valeurs (V), o√π chaque ligne repr√©sente un token. La taille (nombre de colonnes) des matrices de poids (et donc des matrices Q, K, V) est un hyper-param√®tre du mod√®le, mais elle est g√©n√©ralement choisie de telle mani√®re que l'espace de projection soit de dimension r√©duite par rapport √† celui de l'embedding initial. Les vecteurs dans la matrice des Requ√™tes sont confront√©s √† ceux de la matrice des Cl√©s par produit scalaire (mesure de similarit√© entre vecteurs qui tient compte de leur direction et amplitude). Ces valeurs sont ensuite divis√©es par la racine carr√©e de la dimension d'un vecteur Cl√© (r√©duire la variance √©vite la saturation de la fonction softmax et stabilise le gradient). Puis, le r√©sultat de la division passe par une fonction softmax de fa√ßon √† √™tre normalis√© et correspondre √† une distribution de probabilit√©s (tous positifs, ils somment √† 1). Ces valeurs repr√©sentent les scores d'attention. Pour un mot donn√©, le mot ayant le score d'attention le plus √©lev√© correspond au mot le plus informatif pour contextualiser le sens de ce mot dans la s√©quence. L'√©tape suivante consiste √† multiplier ces scores √† la matrice des Valeurs. L'intuition ici est de maintenir la valeur des mots importants (en les multipliant par des valeurs proches de 1) tout en rapprochant celle des autres √† 0 (en les multipliant √† des valeurs proches de 0) afin de permettre au mod√®le de se concentrer sur les parties les plus pertinentes des donn√©es. Finalement, il reste √† agr√©ger ces vecteurs de valeurs pond√©r√©es en les sommant afin de produire la sortie de la couche d'attention, qui fournit ainsi pour chaque mot un vecteur de repr√©sentation contextuellement enrichie.
Dans le Transformer, l'attention est dot√©e d'une sophistication suppl√©mentaire : elle est multi-t√™tes. En bref, l'attention multi-t√™tes applique le m√©canisme d'attention d√©crit ci-dessus plusieurs fois (8 dans Vaswani et al. 2017) de mani√®re simultan√©e, chacune avec des transformations de cl√©s, de requ√™tes et de valeurs diff√©rentes de la m√™me entr√©e, avant de combiner les sorties de chaque t√™te. Cet agencement a pour but de capter des motifs de d√©pendance plus complexes en saisissant simultan√©ment plus de caract√©ristiques et relations dans les donn√©es. Cela peut servir par exemple √† r√©soudre les r√©solutions pronominales (Yvon 2022). 
L'attention peut (doit) √©galement √™tre masqu√©e. Le masquage consiste √† introduire des valeurs extr√™mement basses dans la matrice de scores d'attention avant l'application de la fonction softmax √† certaines positions sp√©cifiques. De la sorte, les positions masqu√©es se voient attribuer des poids d'attention de z√©ro ce qui les rend "invisibles" (i.e. sans influence sur le processus de pr√©diction des mots) (Alammar 2018). Dans un Transformer, il y a g√©n√©ralement de deux sortes de masquage : de remplissage (padding mask) et d'anticipation (look-ahead mask). Le premier, davantage une mesure technique, est utilis√© pour √©viter que le m√©canisme d'attention consid√®re les tokens de remplissage ("<pad>") ajout√©s pour s'assurer que les s√©quences d'un m√™me lot (batch) soient de m√™me longueur lors de l'entrainement. Le deuxi√®me, essentiel pour la g√©n√©ration de texte, vise √† emp√™cher le mod√®le de "tricher" lors de l'entrainement, en lui laissant pr√™ter attention qu'aux mots qui le pr√©c√®dent, de fa√ßon √† imiter la g√©n√©ration de mot de fa√ßon s√©quentielle au moment de l'inf√©rence.
Enfin, l'attention peut √™tre crois√©e (cross-attention) ou auto-dirig√©e (self-attention). Le premier cas correspond √† la situation o√π chaque mot de la s√©quence cible, dans le d√©codeur, "regarde" et int√®gre des informations sur la s√©quence d'entr√©e produite par l'encodeur. Elle est similaire √† l'attention d√©j√† d√©crite dans les mod√®les RNN de type s√©quence-√†-s√©quence et permet en quelque sorte d'aligner la s√©quence de sortie sur la s√©quence d'entr√©e. L'auto-attention a lieu, dans l'encodeur et le d√©codeur, lorsque la s√©quence (d'entr√©e ou de sortie respectivement) se regarde elle-m√™me dans le but d'apprendre les relations au sein de la s√©quence elle-m√™me.
Cette formulation de l'attention dans une √©tape de calculs matriciels "simples" remplace efficacement, pour le traitement du langage tout du moins, les architectures neuronales r√©currentes ou convolutives plus difficiles √† entrainer et sujettes aux probl√®mes de disparition ou explosion de gradients. De surcroit, l'attention de Vaswani et al. (2017) permet une plus grande efficacit√© computationnelle puisque toutes les op√©rations peuvent √™tre effectu√©es en parall√®le. De plus, elle favorise une meilleure compr√©hension du contexte global et des d√©pendances √† longue distance. L'utilit√© de l'attention des Transformer ne se limite √† pas √† son application √† des donn√©es textuelles. Dans l'analyse d'images, elle permet par exemple de porter attention sp√©cifiquement sur certains objets qui les composent. En plus de l‚Äôimpl√©mentation originelle expos√©e ci-dessus bas√©e sur le produit scalaire, il existe diverses variantes de l‚Äôattention (additive, multiplicative, etc. ; Hashimoto 2024c).
1.4.4.	R√©seau √† propagation avant enti√®rement connect√© par position
La sous-couche du r√©seau √† propagation avant (FFN) dense qui suit celle d'attention fait intervenir successivement : une premi√®re transformation lin√©aire, une activation non-lin√©aire (typiquement ReLU), une deuxi√®me transformation lin√©aire. La transformation lin√©aire consiste en la multiplication par une matrice de poids et l'ajout d'un biais (i.e. un neurone ajoutant un poids ajustable et prenant toujours 1 comme valeur d'entr√©e ; Vannieuwenh 2019, p. 285) : la premi√®re augmente la dimensionnalit√© de l'embedding ; la deuxi√®me le ram√®ne √† sa taille √† la sortie de la couche d'attention. Cette s√©quence d'op√©rations, qualifi√©e de position-wise, est appliqu√© √† chaque embedding de token s√©par√©ment et ind√©pendamment des autres √©l√©ments de la s√©quence (bien que les param√®tres soient partag√©s), √©quivalant √† l'effet d'une convolution 1x1. Puisque la couche d'attention en est d√©pourvue, la non-lin√©arit√© apport√©e par la fonction d'activation interm√©diaire est essentielle pour la capacit√© de mod√©lisation du mod√®le.
1.4.5.	Connexions r√©siduelles et normalisation ("Add & Norm")
Les sorties de la couche d'attention sont combin√©es avec les sorties de la couche pr√©c√©dente. Ces connexions r√©siduelles permettent de conserver les informations originales et aident √† stabiliser le gradient en offrant une voie parall√®le sans fonction d'activation pour un meilleur entrainement (Hashimoto 2024c). Pour chaque chaque token de la s√©quence, le r√©sultat de l'addition est ensuite normalis√© (layer normalization), en soustrayant la moyenne et divisant par l'√©cart-type (standardisation) afin de ¬´ cut down on uninformative variation in hidden vector values ¬ª (Hashimoto 2024c). Si la normalisation intervient apr√®s l‚Äôajout des connexions r√©siduelles, on parle alors de post-normalisation. Dans certaines impl√©mentations, la normalisation est appliqu√©e avant l'ajout des connexions r√©siduelles (pr√©-normalisation) (Hewitt 2023). Cette √©tape contribue √† acc√©l√©rer l‚Äôapprentissage (Hashimoto 2024c). Ajuster ainsi les activations √† une distribution normale standard aide principalement √† att√©nuer les probl√®mes de disparition ou explosion de gradients et stabilise l'entrainement (Kaylan 2023).
Apr√®s avoir vu les diff√©rentes briques du Transformer, voyons comment elles s'assemblent dans les deux blocs que sont l'encodeur et le d√©codeur dans le mod√®le propos√© par Vaswani et al. (2017).
1.4.6.	Partie encodeur 
Apr√®s avoir ajout√© une information structurelle en additionnant un plongement positionnel √† la repr√©sentation num√©rique vectorielle du texte en entr√©e, la s√©quence d'entr√©e traverse n couches (n=6 dans l'article original) de construction identique : une sous-couche d'auto-attention multi-t√™tes (8 dans l'article original), suivie par un r√©seau √† propagation avant (FFN) enti√®rement connect√©. Chaque sous-partie de la couche int√®gre une connexion r√©siduelle, c'est-√†-dire que le r√©sultat de chaque sous-partie est additionn√© √† son entr√©e, avant d'√™tre normalis√©.

1.4.7.	Partie d√©codeur
L'architecture causale du d√©codeur est tr√®s similaire √† celle de l'encodeur, hormis deux adaptations majeures. Dans chacune des couches, une sous-couche d'attention crois√©e (√©galement multi-t√™tes) est ins√©r√©e entre la normalisation qui suit l'auto-attention et le FFN. Cet ajout permet au d√©codeur d'incorporer des informations de la s√©quence source pendant la g√©n√©ration de la s√©quence cible. Ensuite, l'auto-attention dans cette partie du Transformer est masqu√©e afin d'assurer une g√©n√©ration de texte autor√©gressive (le look-ahead masking cachant les tokens futurs). 
Apr√®s la sortie de la normalisation par couche qui fait suite au r√©seau √† propagation avant, les sorties du d√©codeur passent par une couche lin√©aire de projection qui ajuste leur dimension √† la taille du vocabulaire (ou au nombre de classes si le mod√®le est entrain√© en vue d'une t√¢che de classification). Gr√¢ce √† cette transformation affine, un vecteur comportant un score (logit) pour chacun des mots est ainsi obtenu. Il ne reste qu'√† appliquer √† scores une fonction softmax pour les convertir en une probabilit√© proportionnelle √† leur exponentielle. Le mot associ√© √† la probabilit√© la plus haute correspond au mot √† ajouter √† la s√©quence en cours de g√©n√©ration. √Ä chaque √©tape de g√©n√©ration de texte lors de l'inf√©rence, les mots produits sont r√©introduits en entr√©e du d√©codeur, permettant ainsi la construction s√©quentielle de la sortie. A l'entrainement, le teacher forcing d√©cale la s√©quence vers la droite avant de la fournir au d√©codeur pour que la pr√©diction se base uniquement sur les mots pr√©c√©dents.
Une subtilit√© suppl√©mentaire, qui n‚Äôest pas propre aux Transformers, s'ajoute au processus de g√©n√©ration de texte. En effet, si seul le mot le plus probable √©tait choisi en sortie (greedy search ou greedy decoding), des phrases de probabilit√© tr√®s proche seraient d'embl√©e √©cart√©es et ¬´ if we make a mistake at one time step, the rest of the sentence could be heavily impacted ¬ª (Genthial et al. 2019). Sans consid√©rer d'autres variantes, le texte entier obtenu en fin de compte pourrait manquer de coh√©rence et/ou d'optimisation globale. Une m√©thode souvent utilis√©e est la recherche en faisceaux ou beam search, qui consiste √† conserver √† chaque √©tape de la g√©n√©ration, les k solutions les plus probables (k=1 √©quivaut √† la recherche greedy ou gloutonne). Par exemple pour k=2 : √† la premi√®re position, les deux tokens les plus probables sont conserv√©s ; le mod√®le est ex√©cut√© s√©par√©ment pour chacun de ces tokens ; apr√®s les deux ex√©cutions du mod√®le, sur l'ensemble des s√©quences produites on ne garde √† nouveau que les deux associ√©es aux probabilit√©s les plus √©lev√©es ; pour chacune de ces deux s√©quences, le mod√®le est √† nouveau ex√©cut√©, et ainsi de suite (Doshi 2021). Des valeurs de k plus √©lev√©es am√©liorent (de fa√ßon non monotone) la pr√©cision et k peut √™tre choisi de fa√ßon √† combiner performance acceptable et efficacit√© computationnelle. De ce fait, cette m√©thode est la plus utilis√©e pour les syst√®mes de traduction neuronale (Genthial et al. 2019). Les valeurs de k sont g√©n√©ralement choisies entre 3 et 6 (4 pour GPT-3 ; Zhao et al. 2023). D'autres m√©thodes permettent d'influencer le choix des mots et rendent la g√©n√©ration de texte plus modulables. Un param√®tre fr√©quemment utilis√© est la temp√©rature, un coefficient int√©gr√© au d√©nominateur de l'exponentielle dans la formule softmax pour ajuster la distribution de probabilit√©s des tokens. Une valeur √©gale √† 1 reste sans effet sur la distribution de probabilit√©s. Lorsque les valeurs tendent vers z√©ro, la distribution des probabilit√©s devient de plus en pointue, se concentrant autour des tokens les plus probables. A valeur nulle, la s√©lection est enti√®rement d√©terministe et √©quivaut √† une recherche gloutonne. A l'inverse, plus les valeurs d√©passent 1, plus la courbe s'aplatit et s'uniformise : l'incertitude sur le choix du token s'accroit ; le texte g√©n√©r√© est plus diversifi√©, le mod√®le plus "cr√©atif".

1.4.8.	Encodeur et/ou d√©codeur ?
L'architecture encodeur-d√©codeur permet de pr√©dire du texte mot par mot en fonction de l'entr√©e. Une configuration particuli√®rement ad√©quate pour les t√¢ches de traduction, sc√©nario pr√©sent√© dans l'article fondateur de Vaswani et al. (2017). D'autres t√¢ches de conversion texte-√†-texte dans lesquelles ils performent consistent par exemple √† corriger, r√©sumer ou paraphraser un texte de m√™me que g√©n√©rer une r√©ponse √† une question. Un autre exemple est BART (Lewis et al. 2020). Ce mod√®le seq2seq qui combine encodeur bidirectionnel et d√©codeur autor√©gressif, est entrain√© √† reconstruire la phrase d'entr√©e bruit√©e. Propos√© plus r√©cemment par Google (Raffel et al. 2020), T5 a √©t√© entrain√© sur plusieurs t√¢ches supervis√©es ou non. 
Par sa modularit√©, l'architecture Transformer offre une flexibilit√© suffisante pour utiliser l'encodeur ou le d√©codeur de fa√ßon ind√©pendante en fonction du but recherch√©. Un mod√®le encoder-only permet l'apprentissage de repr√©sentations riches et contextuelles des donn√©es d'entr√©e. Celles-ci peuvent ensuite √™tre utilis√©es pour accomplir des t√¢ches de compr√©hension de langage (i.a. classification de textes, extraction d'entit√©s ou de relations ; (Kaylan 2023). BERT (Bidirectional Encoder Representations from Transformers ; Devlin et al. 2019) en est suppos√©ment l'arch√©type le plus fameux. Selon Bommasani et ses co-auteurs (Bommasani et al. 2021), BERT marqua m√™me ¬´ a sociological inflection point ¬ª, ce mod√®le √©tant devenu la norme pour les t√¢ches de TAL et annonce l'entr√©e dans l'√®re des mod√®les de fondation. Ce mod√®le de langue par masquage pr√©sent√© en 2018 par Google AI marque une avanc√©e notable dans le domaine du TAL pour la repr√©sentation profonde bidirectionnelle des mots (i.e. prenant en compte le contexte ant√©rieur (gauche) et post√©rieur (droite)). Avec plus de 300 millions de param√®tres pour sa version la plus grande (BERT-large : 24 couches, 16 t√™tes d‚Äôattention ; Hashimoto 2024d), il a √©t√© con√ßu comme un mod√®le pr√©-entrain√© de fa√ßon non-supervis√©e sur un corpus de donn√©es massif (4 mia de tokens issus de Wikipedia Anglais (2‚Äô500 mio de mots) et BookCorpus (800 mio de mots), soit env. 16 GB de texte non compress√© ; Hashimoto 2024d, Sagot 2023d). Si le pr√©-entrainement fut tr√®s demandeur en ressources (4 jours de calculs sur 64 chips TPU), le fine-tuning peut √™tre mis en ≈ìuvre de fa√ßon pratique sur 1 seul GPU selon le principe ¬´ pretrain once, finetune many times ¬ª (Hashimoto 2024d). Deux t√¢ches sont apprises durant l'entrainement. La premi√®re, la pr√©diction d'un mot masqu√©, permet de cr√©er une repr√©sentation au niveau du token ; la deuxi√®me, la pr√©diction de la probabilit√© que deux phrases se suivent ou ne se suivent pas, capte les d√©pendances entre les phrases (Guimar√£es et al. 2024). En plus des plongements lexicaux des tokens, BERT utilise les repr√©sentations cach√©es du token sp√©cial [CLS] comme plongement de phrases, utiles par exemple pour des t√¢ches de classification. Il se destine √† √™tre affin√© en lui adjoignant une seule couche de sortie suppl√©mentaire pour l'entrainer sur des t√¢ches sp√©cifiques (par exemple : √©tiquetage de tokens ou de phrases, analyse de sentiments) (Devlin et al. 2019). Jusqu'√† aujourd'hui, BERT continue d'engendrer de nombreux "descendants" (MultilinguialBERT, couvrant plus de 100 langues ; CamemBERT ou FlauBERT, entrain√©s sur des donn√©es fran√ßaises ; etc.) (Sagot 2023d). 
La partie d√©codeur produit un texte de mani√®re auto-r√©gressive (mod√®le de langue). Cela sied √† des situations o√π le texte √† g√©n√©rer n'a pas besoin d'√™tre directement align√© sur la s√©quence d'entr√©e. Les mod√®les au c≈ìur des agents conversationnels comme ChatGPT sont donc fr√©quemment de ce type. Ce type de mod√®les pr√©dominent actuellement les grands mod√®les de langue (e.g. GPT-4, PaLM, LLaMA ; Zhao et al. 2023). Nous abordons en d√©tail les mod√®les GPT au chapitre suivant.
1.4.9.	Avantages et limites des Transformer

L‚Äôavanc√©e majeure port√©e par les Transformers et leur m√©canisme d‚Äôattention est d‚Äôavoir rendu possibles, gr√¢ce √† la fa√ßon dont est impl√©ment√©e l‚Äôattention, la parall√©lisation des calculs (s√©quences enti√®res trait√©es en un coup) ainsi que l‚Äôaugmentation de la port√©e du contexte (Sagot 2023d). En offrant la possibilit√© au d√©codeur de ¬´ regarder ¬ª directement la s√©quence source plut√¥t que de se baser uniquement sur un vecteur de caract√©ristiques de taille fixe, l‚Äôattention solutionne le bottleneck problem des mod√®les seq2seq ant√©rieurs (Hashimoto 2024c). L‚Äôinspection de la distribution de l‚Äôattention apporte en outre une certaine explicabilit√© (Hashimoto 2024c).
L'attention par produit scalaire normalis√© ici d√©crite (de type full attention) n'est pas exempte d'inconv√©nients. De complexit√© quadratique par rapport √† la longueur de s√©quence en temps et en m√©moire, elle repr√©sente une limitation majeure pour le traitement de longues s√©quences et impacte l'entrainement autant que l'inf√©rence (Yvon 2022). Il y a donc une recherche active en vue d'optimiser cette √©tape pour r√©duire la complexit√© computationnelle et de nombreuses variantes ont √©t√© propos√©es en vue de la rendre lin√©aire (Sagot 2023d). L'attention √©parse (sparse attention), o√π ¬´ instead of the whole sequence, each query can only attend to a subset of tokens based on the positions ¬ª (e.g. Factorized Attention dans GPT-3), en est une parmi de nombreuses autres (multi-query attention, FlashAttention, PagedAttention, etc.) (Zhao et al. 2023). Une autre voie de recherche s‚Äôatt√®le √† combiner Transformers et RNNs (e.g. RetNet ; Sun et al. 2023) en vue de tirer parti du meilleur de chacune de ces architectures (respectivement : encodage d‚Äôune s√©quence enti√®re en parall√®le et d√©codage plus efficace bas√© sur l‚Äô√©tat pr√©c√©dent uniquement) (Zhao et al. 2023).
Par ailleurs, ces mod√®les, requ√©rant souvent des ajustements techniques, restent difficiles √† entrainer (Sagot 2023d). Tout un pan de la recherche actuelle vise √† les am√©liorer √† l‚Äôaide de m√©thodes innovantes autant pour les couches de la normalisation, l'initialisation des param√®tres et de nombreux autres adaptations (Zhao et al. 2023 ; Wan et al. 2023). 
Un dernier point d‚Äôattention concerne le corpus d‚Äôentrainement. Pour atteindre de bonnes performances, l‚Äôapprentissage des Transformers actuels doit se baser sur des quantit√©s de donn√©es gargantuesques. De surcroit, un √©quilibre doit √™tre trouv√© entre la taille des mod√®les et la taille (diversit√©) des corpus, qui doivent croitre en parall√®le (Sagot 2023d). Comme illustr√© par Chinchilla (Hoffmann et al. 2022), le nombre de param√®tres ne fait pas tout : avec ¬´ seulement ¬ª 70 mia de param√®tres, ce mod√®le battait pour de nombreuses t√¢ches d‚Äô√©valuation nombre de mod√®les dot√©s de plusieurs centaines de milliards de param√®tres. 

1.4.10.	Transformers, LLMs et changements de paradigmes
Dans une section pr√©c√©dente, nous avons bri√®vement retrac√© le d√©veloppement des m√©thodes de TAL, des approches symboliques aux r√©seaux de neurone profonds modernes. A leur d√©but, un apport majeur de ceux-ci fut leur capacit√© √† repr√©senter les mots de fa√ßon distribu√©e et √† utiliser ces vecteurs pour la pr√©diction de texte. De la sorte, √† consacrer une utilisation ne se limitant plus √† la seule mod√©lisation de s√©quences mais s'√©tendant √† l'apprentissage de repr√©sentations (representation learning ; Zhao et al. 2023). Avec la popularisation des Transformers, d‚Äôautres paliers furent franchis dans la mod√©lisation du langage. 
Tout d'abord, le succ√®s des mod√®les de langue pr√©-entrain√©s (PLMs). D√©j√† usit√© bien avant les LLMs, le pr√©-entrainement permet de commencer l'entrainement sur la t√¢che sp√©cifique avec les valeurs de poids obtenues lors du pr√©-entrainement plut√¥t qu'une initialisation al√©atoire (Charniak 2021, p. 128). De fa√ßon importante, le pr√©-entrainement peut √™tre men√© sur un vaste corpus de donn√©es non √©tiquet√©es avant de se focaliser sur une t√¢che sp√©cifique pour laquelle un nombre restreint de donn√©es sont disponibles (Yvon 2022). Quand il s‚Äôagit de donn√©es textuelles, la phase de pr√©-entrainement permet par exemple au mod√®le d'acqu√©rir des notions sur le contexte, la grammaire, le vocabulaire de m√™me que des connaissances g√©n√©rales (Hadi et al. 2023). Le BiLSTM ELMo (Peters et al. 2018) y faisait d√©j√† recours et passait au mod√®le sp√©cifique en aval les repr√©sentations apprises dans la phase initiale (feature-based approche) (Devlin et al. 2019). Avec BERT et GPT c‚Äôest la combinaison pr√©-entrainement suivi d‚Äôajustement fin (fine tuning) qui est √©rig√©e comme paradigme d'apprentissage (Kaylan 2023 ; Liu et al. 2023a) ; Zhao et al. 2023). D√©j√† utilis√© avec succ√®s en vision par ordinateur, le fine-tuning ou affinement r√©duit le besoin en donn√©es √©tiquet√©es √† la phase d'ajustement (sp√©cifique √† chaque t√¢che), qui en requiert d√®s lors une quantit√© bien moindre, l'acquisition de connaissances g√©n√©rales s'op√©rant de mani√®re non supervis√©e lors du pr√©-entrainement (Kaylan 2023). A noter que cette pr√©valence du pr√©-apprentissage non-supervis√© a √©galement des implications sur la fa√ßon dont se conduit le travail des d√©veloppeurs : requ√©rant moins d'a priori structurels (structural priors) et √©tant tr√®s gourmand en ressource, les possibilit√©s d'exp√©rimenter des variantes architecturales s'en retrouvent restreintes (Liu et al. 2023a).
Apr√®s des √©tudes pionni√®res d√©crivant les premiers PLMs a suivi une phase de recherche constatant l'am√©lioration de ces mod√®les avec la mise √† l'√©chelle des mod√®les et/ou des donn√©es. Ainsi advint l'√®re des grands mod√®les de langue (LLMs; e.g. GPT-3, PaLM, LLaMA) aux capacit√©s surprenantes et capables d'accomplir des t√¢ches complexes. Essentiellement bas√©s sur une architecture Transformer comme les PLMs, ils s'en distinguent par un nombre incommensurable de param√®tres (centaine de milliards), de calculs et la quantit√© astronomique de donn√©es utilis√©es pour l'entrainement (Kaylan 2023 ; Zhao et al. 2023). La fronti√®re entre PLMs et LLMs demeure floue, les valeurs de seuil n'√©tant pas pour l'heure clairement d√©finies et la terminologie ne semble pas faire l'objet de consensus parmi la communaut√© scientifique. Comme d‚Äôautres auteurs, Raiaan et ses coll√®gues (Raiaan et al. 2024) font remonter le premier grand mod√®le de langue au syst√®me de traduction automatique d√©voil√© par Google en 2015 (Google's Neural Machine Translation system ; Wu et al. 2016). Bommasani et ses co-auteurs ont regroup√© sous le terme de mod√®le de fondation (foundation model) ce type de mod√®les g√©n√©ralistes, ¬´ trained on broad data (generally using self-supervision at scale) that can be adapted (e.g., fine-tuned) to a wide range of downstream tasks ¬ª (Bommasani et al. 2022) et ¬´ distinct from narrow AI systems, which are trained for one specific task and context ¬ª (Ada Lovelace Institute 2023).
Surtout, les LLMs sont caract√©ris√©s par des capacit√©s √©mergentes (encore mal comprises) qui ne se manifestent pas √† plus petite √©chelle. Parmi celles-ci, Zhao et ses coll√®gues en citent trois : "in-context learning [,] instruction following [and] step-by-step reasoning" (Zhao et al. 2023). Le paradigme de l'apprentissage en contexte (in-context learning) fait r√©f√©rence √† "a new learning paradigm that does not require task-specific fine-tuning and many labelled instances" (Kaylan 2023). La mise au point des LLMs inclue g√©n√©ralement une phase d'alignement dite de m√©ta-entrainement (meta-training ; Kaylan 2023) pour conformer les sorties du mod√®le aux valeurs et pr√©f√©rences attendues des utilisateurs (Kaylan 2023) (voir section 6.1.).
Les LLMs se montrent capables de performances dans (quasiment ?) toutes les t√¢ches de TAL - des questions-r√©ponses √† la g√©n√©ration, classification et synth√®se de texte, en passant par la traduction, l'extraction d'informations ou la reconnaissance de dialogues. Mais l'objectif des mod√®les de langue a √©volu√© au fil de ces d√©veloppements, et ne se limite d√©sormais plus √† la "simple" mod√©lisation du langage puisqu‚Äôil s'est √©largi √† la r√©solution de t√¢ches complexes (Zhao et al. 2023). Liu et ses co-auteurs (Liu et al. 2023a) font remonter √†2021 un nouveau tournant qu'il r√©sume √† "pre-train, prompt, predict". Plut√¥t que de sp√©cialiser les mod√®les pr√©-entrain√©s en adaptant les fonctions objectives, les t√¢ches en aval sont formul√©es au moyen d'une requ√™te textuelle (prompt) de fa√ßon √† correspondre √† celles r√©solues lors de la phase d'entrainement initiale. De nouvelles pratiques voient le jour en remplacement du fine-tuning, par exemple le few-shot prompting, qui consiste √† "includ[e] a few examples selected from the dataset in the prompt, to help the model better understand task requirements" (en l‚Äôabsence de tout exemple, on parle de zero-shot prompting ; Cao et al. 2023). Ainsi, sans passer par un entrainement sp√©cifique supervis√©, leur seul apprentissage non-supervis√© permet √† ces mod√®les de r√©soudre des t√¢ches tr√®s vari√©es. 

Au final, l'expansion progressive des capacit√©s des mod√®les de langue peut ainsi √™tre r√©sum√©e : les premiers LMs statistiques se restreignaient √† des t√¢ches sp√©cifiques (e.g. recherche d'informations), les premiers NNs furent capables d'apprendre des caract√©ristiques de fa√ßon agnostique, de "bout-en-bout" (se passant de feature engineering "manuel"), les PLMs apprirent des repr√©sentations contextualis√©es √† optimiser pour les t√¢ches en aval, les LLMs b√©n√©ficient de l'effet d'√©chelle et "can be considered as general-purpose task solvers" (Zhao et al. 2023). En outre, les LLMs s'accompagnent √©galement de bouleversements pratiques : ils sont d√©sormais accessibles par une interface de prompts et requi√®rent des capacit√©s mat√©rielles et techniques in√©dites pour √™tre entrain√©s et maintenus (Zhao et al. 2023).

{% include links.html %}
