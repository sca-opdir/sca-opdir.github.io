---
title: La famille de modèles GPT
keywords: IA, IT, ChatGPT
last_updated: June 18, 2024
tags: [IA]
summary: "ChatGPT Plus : plugins et GPTs"
sidebar: mydoc_sidebar
permalink: ChatGPT_modelesGPT.html
folder: mydoc
---

5.	La famille de modèles GPT

La paternité (ou maternité) des modèles GPT revient à OpenAI. Cette société a vu le jour en 2015 comme organisme de recherche à but non lucratif. Parmi ses fondateurs figurent des personnages renommés de la sphère technologique : Elon Musk (Tesla), Sam Altman (Y Combinator), Reid Hoffman (LinkedIn), Peter Thiel (Palantir) et Brockman (Stripe) (Dale 2021). Cette société érige comme mission que l'IA bénéficie à l'ensemble de l'humanité et aspire à être la pionnière de l'IA générale. Entre autres, elle bénéficie d’investissements massifs de Microsoft tout comme de ses services de cloud (https://www.ictjournal.ch/news/2023-01-24/cest-confirme-microsoft-va-investir-des-milliards-dans-openai-update). 
Basés sur la partie décodeur du Transformer, les modèles de la famille GPT (Generative Pre-trained Transformer) sont de type génératif et produisent un token l'un après l'autre, suivant ainsi la définition "traditionnelle" d'un modèle de langue (prédiction du mot suivant). Dans cette configuration, chaque mot produit est ajouté à la séquence en entrée, et cette nouvelle séquence devient l'entrée du pas de temps suivant (modèle auto-régressif, à l'instar des RNNs). D’architecture comparable, c’est surtout l’augmentation du nombre de paramètres et du volume du corpus d’entrainement qui marque l’évolution de GPT-1 à GPT-4. A noter que la dénomination GPT-'X', le modèle canonique s'accompagne en fait d'une ribambelle de versions (par exemple : GPT-3 Small, GPT-3 Medium, GPT-3 XL etc. qui correspondent généralement à des variations dans la taille des modèles).
5.1.	GPT-1

La première version des modèles GPT, GPT-1 (Radford et al. 2018), était de taille comparable à BERT (117 millions de paramètres). Nonobstant la terminologie actuelle, l’acronyme « GPT » ne figurait pas dans l’article original (Hashimoto 2024d).
Basé sur une architecture décodeur faite de 12 couches et entrainé comme modèle de langue causal génératif, il marquait déjà de notables améliorations dans le domaine du TAL et un premier pas vers des modèles constitués d'un nombre pléthorique de paramètres (Kaylan 2023). Ses données d’entrainement étaient le BooksCorpus, rassemblant plus de 7'000 livres (4.6 GB de texte, 1 mia de tokens ; Yang 2024a). Cette première génération de modèles constitua l’une des premières preuves que la modélisation du langage à grande échelle peut être une technique de pré-entrainement efficace (Hashimoto 2024d).
5.2.	GPT-2

GPT-2, sorti en 2019 (Radford et al. 2019), est de construction similaire à GPT-1 (à l'exception notamment de quelques modifications dans les couches de normalisation) tout en atteignant les 1.5 milliards de paramètres, dont l'estimation repose sur un entrainement nécessitant 40 GB de texte (Yang 2024a), soit environ 10 mia de tokens. Par rapport à GPT-1, le volume des données d'entrainement a donc été décuplé, incluant entre autres les corpus WebText (38 GB introduit par OpenAI) et CommonCrawl (570 GB). Elles ont notamment été étoffées avec davantage de données d'Internet validées par des humains (e.g. posts Reddit de qualité validée par les utilisateurs). En outre, d'autres interventions interviennent, par exemple concernant la longueur du contexte d'entrée (1024 plutôt que 512 tokens), les méthodes de normalisation, la taille des batch pendant l'entrainement, ou celle du vocabulaire. Enfin, GPT-2 est le dernier modèle de la série GPT dont le source code encore disponible sur GitHub (https://github.com/openai/gpt-2). Ce ne sera plus le cas de GPT-3 et des suivants, pas plus que les valeurs des paramètres des modèles entrainés.
De façon importante, GPT-2 introduit l'apprentissage multi-tâches de façon non-supervisée (ou multi-task training : « [a] meta-learning perspective [...] each task is viewed as a training example » ; Boulanger 2023). En effet, les auteurs proposent que la plupart des tâches de TAL peuvent en fait être reformulées comme des requêtes en langage naturel et qu'elles ont déjà été abordées et résolues. Des tâches diverses peuvent ainsi être accomplies en passant par la modélisation de la langue : avec la formulation p(sortie|entrée, tâche) (prédiction de la sortie conditionnée par l'entrée et la tâche à accomplir), « language text can be naturally employed as a unified way to format input, output and task information [and] the process of solving a task can be cast as a word prediction problem for generating the solution text » (Zhao et al. 2023). Ainsi, la génération de texte demeure l'objectif de l'apprentissage spécifique à la tâche autant qu'il était celui de la modélisation non supervisée. Toutefois, durant le pré-entrainement, il est conditionné par les mots précédents et cherche à prédire le mot suivant (vanilla text generation) ; alors qu'au moment de l'inférence il est conditionné par le prompt et vise à générer les prochains mots (conditional text generation) (Kaylan 2023). Bien que cela n'était pas encore nommé ainsi, GPT-2 utilisait donc de l'in-context learning pour parvenir à l'apprentissage de tâche non supervisé sans mise à jour des paramètres (Zhao et al. 2023). Il démontrait par ailleurs des capacités émergentes d’apprentissage sans exemple (zero-shot learning). Ceci nécessite toutefois que le corpus d'entrainement doit être suffisamment diversifié afin d'être capable de généralisation et d'accomplir un plus grand nombre de tâches.
En termes de performances, GPT-2 battait déjà des records dans des tâches de modélisation de langue, même sans affinement à des tâches spécifiques (Yang 2024a). Conçu pour être un « unsupervised multitask learner »(Radford et al. 2019), GPT-2 ne battait pas les modèles supervisés d'alors (Zhao et al. 2023). Les évaluations de GPT-2 suggéraient également que le modèle était sous-ajusté (underfitted) et qu'un entrainement plus poussé aurait permis d'améliorer la perplexité (une mesure d'évaluation pour les modèles de langue qui correspond à l’« inverse probability of [the] corpus, according to [the] language model[,] normalized by number of words » ; Hashimoto 2024a). Cette observation poussa vers la mise au point de modèles encore plus grands (Kaylan 2023). La concurrence et la course aux modèles gargantuesques sont effrénées - la même année le Megatron-LM de NVIDIA atteint les 8.3 mia de paramètres (Shoeybi et al. 2019).
5.3.	GPT-3

OpenAI maintient le rythme et le successeur de GPT-2, logiquement nommé GPT-3 (Ouyang et al. 2022) sort en 2020 et affole les compteurs. Plus grand LLM de son époque (175 mia de paramètres, 96 couches ; détrônant T5 et ses 11 mia de paramètres ; Hashimoto 2024d), entrainé sur un corpus de données massif (300 mia de tokens, plus de 600 GB de données ; Hashimoto 2024d), GPT-3 marque un saut en termes de performances et de capacités de cohérence et de fluidité (Raiaan et al. 2024). Pour se figurer la "démesure" d'un tel modèle, le temps d'apprentissage de GPT-3 sur un ordinateur de gaming aurait duré plus de 350 ans (Pérez-Ortiz et al. 2022). Et le temps de lecture de son corpus d’entrainement prendrait à un humain environ 20'000 ans (Sagot 2023a).
Le modèle et son architecture sont semblables à ceux de GPT-2 (Brown et al. 2020), hormis quelques ajustements (par exemple, alternance d'attentions denses et éparses). Une fois de plus, le corpus d'entrainement est élargi (par exemple WebText2, version augmentée de WebText, contenant des textes jugés de haute qualité comme ceux de Wikipédia). Il semble émerger un effet d'échelle car les performances de GPT-3 s’avèrent nettement supérieures à celles de GPT-1 et GPT-2, en apprenant seulement à partir du contexte sans mise à jour des paramètres (« learning without gradient steps » ; Hashimoto 2024d). Des capacités rendues possibles notamment par la taille du modèle et la diversité du corpus d'entrainement.
GPT-3 permet de générer tout ce qui a une structure textuelle de façon plus robuste que ses prédécesseurs (y compris du code informatique, bien qu'il n'ait pas été entrainé pour cela), affichant des améliorations remarquables même sur des sujets "de niche" (Badri et al. 2023). Avec leur capacité de few-shot learning, one-shot learning, ou zero-shot learning (apprentissage basé respectivement sur quelques-uns, un ou aucun exemples), les modèles GPT-3, développés avec le but explicite de résoudre des tâches par conditionnement, marquent un changement de paradigme dans la spécialisation des LLMs (Kaylan 2023). Alors que jusqu'à GPT-2 les modèles étaient fiables avant tout lorsqu'ils étaient couplés à de l'apprentissage supervisé en aval, GPT-3 propulse l'apprentissage en contexte (in-context learning, déjà présent mais de façon limitée depuis GPT-1) comme moyen efficace de spécialiser un modèle généraliste sans calcul de gradient. Néanmoins, pour certaines tâches complexes, l’apprentissage qui passe par le prompt peut s’avérer ardu. Pour résoudre ces difficultés, les techniques de prompting s’affinent, par exemple en vue d’accomplir un raisonnement par étapes (chain-of-thought prompting ; Wei et al. 2022}).
 
5.4.	GPT-4

Dernier numéro de la série GPT à l’heure d’écrire ces lignes, GPT-4 est sorti en mars 2023 et est actuellement intégré à la version payante de ChatGPT (20 dollars par mois). Les détails de son architecture Transformer n'ont pas été divulgués, mais GPT-4 renferme sans doute encore davantage de paramètres (1500 mia selon Delbecq 2024). Et alors que la fenêtre de contexte des modèles GPT-3 atteignait au maximum 4096 tokens, celle des GPT-4 s’élève jusqu’à 128'000 (https://platform.openai.com/docs/models/overview).
Plus performante que ses prédécesseurs, cette version aurait, selon OpenAI, 82% de moins probabilité de répondre à du contenu interdit et 40% de plus de répondre une réponse factuelle que GPT-3.5 (https://openai.com/gpt-4). Il serait également moins sensible aux variations dans les prompts, plus créatif et plus apte à comprendre des instructions nuancées. OpenAI souligne l'accent mis sur la sécurité et la fiabilité de leur modèle. Surtout, GPT-4 est capable de traiter des images (multi-modalité) et intègre DALL-E (une IA également développée par OpenAI) pour leur génération. Par ailleurs, GPT-4 résout mieux les tâches complexes que GPT-3.5 ; il est également plus robuste pour contrer les questions malicieuses ou provocatrices car il utilise un signal de récompense pour la sécurité supplémentaire lors de la phase d'entrainement RLHF (voir section suivante ; Zhao et al. 2023).
Certains indices laissent penser un développement prochain vers la gestion de la parole (https://www.heidi.news/cyber/gpt-4-apres-le-texte-et-l-image-l-ia-veut-prendre-votre-voix). D'autres développements avec des améliorations techniques ont déjà été communiquées (e.g. GPT-4V, GPT-4 turbo avec une fenêtre de contexte de 128K), mais ne seront pas discutées davantage dans ce rapport.


{% include links.html %}
