---
title: Architectures neuronales pour le TAL
keywords: IA, LLM, TAL
last_updated: June 18, 2024
tags: [IA, LLM, TAL]
summary: "Evolution des architectures pour le TAL jusqu'aux LLMs"
sidebar: mydoc_sidebar
permalink: LLM_TAL_architectures.html
folder: mydoc
---

1.	Architectures neuronales pour le TAL

1.1.	Principes de fonctionnement des réseaux de neurone

La description mathématique de réseaux de neurones auto-organisés remonte à 1948 et Alan Turing déjà (Vannieuwenh 2019, p.24). Le perceptron, premier réseau de neurones à couches, capable d'apprentissage supervisé, est décrit en 1957 par Rosenblatt (Vannieuwenh 2019, pp. 25, 261). Un unique neurone formel n'étant capable de séparer uniquement des données linéairement séparables, dans la pratique plusieurs neurones sont combinés en réseau multicouches, constitués de nombreuses couches intermédiaires dites cachées (Vannieuwenh 2019, p. 313). En utilisant des fonctions d'activation non linéaires (d’abord sigmoïdales ou logistiques, puis désormais fréquemment ReLU), un modèle paramétrique hautement non linéaire peut ainsi être mis au point (Azencott 2019, p. 98). Un réseau de neurones est composé d’une séquence de fonctions arrangées en couches où les sorties d'une couche servent d'entrée à la suivante. Ainsi structuré, le modèle peut être complexe et flexible, opérant de la sorte comme un approximateur universel.
Nous n'allons pas revenir dans les détails sur le fonctionnement des réseaux de neurones. Rappelons ici simplement que l'apprentissage émane de l'alternance répétée, pour chaque observation, d'une phase de propagation avant (somme pondérée des entrées des neurones soumise à une fonction d'activation afin d'obtenir une valeur de sortie du neurone) et de rétropropagation des erreurs (parcours du réseau en sens inverse afin d'ajuster les poids des neurones en tenant compte de la différence entre la valeur attendue et la valeur réalisée telle que calculée par une fonction de perte) (Vannieuwenh 2019, p. 279 ; Azencott 2019, p. 101). Initialisés avec des valeurs le plus souvent aléatoires, les paramètres du modèle (i.e. les poids des connexions du réseau) sont ajustés lors de l’apprentissage en vue de minimiser la fonction d'erreur généralement par descente de gradient, par itérations successives jusqu'à convergence (minimum d'erreur). Le gradient de la fonction d'erreur est d’abord calculé par rapport à chaque poids, indiquant la direction et l'ampleur de la modification nécessaire pour réduire l'erreur. Les poids sont ensuite mis à jour dans la direction opposée au gradient calculé, avec une magnitude déterminée par le taux d'apprentissage. Le gradient correspond à la pente de la fonction de perte (Charniak 2021, p. 13). Cet algorithme du gradient, qui appartient à la famille des algorithmes à directions de descente (Azencott 2019, p. 210), peut être imagé ainsi : imaginons que l'on souhaite descendre de la montagne en direction de la plaine, le gradient correspondant à la pente du sol sur lequel nous avançons pas à pas en direction du bas jusqu'à attendre la vallée. Le "pas" porte également le nom de taux d'apprentissage, un hyperparamètre du modèle (Vannieuwenh 2019, pp. 113, 279, 282). A côté de ce dernier, de nombreux autres choix (initialisation des paramètres, standardisation des données, fonctions d'activation, etc.) conditionnent l'atteinte d'un minimum global, qui ne peut par ailleurs jamais être garantie le problème d'optimisation n'étant pas convexe (Azencott 2019, pp. 101, 104). En mettant à profit la règle de dérivation en chaine (ou règle de dérivation des fonctions composées) et la mémoïsation, les calculs des gradients permettant l'actualisation des poids sont très efficaces (Azencott 2019, p. 102), assurant la popularité de l'entrainement par rétropropagation. Un algorithme de descente de gradient "strict" impliquerait de calculer les modifications des valeurs des paramètres pour chaque donnée d'entrainement avant de procéder à leur mise à jour. Dans la pratique, cela n'est pas possible en raison de la masse trop volumineuse de données utilisées pour l'entrainement du modèle. Pour cette raison, c'est un algorithme de descente de gradient dit stochastique qui est habituellement mis en œuvre et actualise les valeurs des paramètres après avoir traité un lot (batch) d'exemples du jeu d'entrainement (Charniak 2021, p. 16).
Si ces principes de base sont partagés par la plupart des réseaux de neurones, la créativité des spécialistes a engendré une grande diversité dans la façon d’agencer les connexions entre les neurones et la complexification des calculs qui y sont opérés. Dans les sections qui suivent, nous passons en revue quelques architectures qui marquèrent l’évolution du deep learning pour le traitement de données textuelles. Ces évolutions ont précédé les Transformers des modèles GPT et amené progressivement les concepts sur lesquels ces derniers reposent et les briques dont ils se composent. Nous les couvrirons de manière superficielle sans rentrer dans les détails mathématiques mais en essayant (avec l’aide de ChatGPT) d'en restituer l'intuition qui les sous-tend.

1.2.	Les réseaux à propagation avant (FFNs) et word embeddings

Dans les réseaux à propagation avant (FFNs), aussi appelés perceptrons multicouches (MLPs), l'information circule uniquement dans une direction à travers plusieurs couches de neurones. Leur architecture simple est à la base des premiers réseaux de neurones utilisés dans le TAL. Le travail de Bengio et ses collègues (Bengio et al. 2003) est précurseur des techniques modernes d'apprentissage auto-supervisées (étiquettes générées à partir des données elles-mêmes, se passant d'annotations manuelles externes) et des réseaux neuronaux profonds dans le TAL. Le modèle de langue est ici formulé tel un problème de classification (prédire un symbole parmi l'ensemble du vocabulaire à partir des mots le précédant), que Bengio et al. (2003) proposent de résoudre par un réseau de propagation avant, qui marque « le passage de modèles discrets à des représentations numériques » (Yvon 2022). Leur modèle démontrait de meilleures performances que les standards des méthodes statistiques de l'époque, les modèles n-gramme (avec généralement n=3, i.e. deux mots de contexte), et apportait une réponse aux problèmes de sparsity et storage de ces derniers évoqués plus haut (Hashimoto 2024a).
Basé sur une architecture MLP, le réseau de neurones apprend une fonction d'encodage non-linéaire qui compresse « le contexte (les mots précédents) dans un vecteur [dense] de faible dimension, à partir duquel la distribution conditionnelle des mots successeurs est calculée » (Yvon 2022). En parallèle, la projection du vocabulaire dans un espace de dimension réduite constitue son plongement lexical. De la sorte, cet article pionnier introduit une architecture neuronale efficace pour des tâches de TAL et de façon concomitante la notion de représentation distribuée des mots (distributed representation ; en contraste avec les représentations discrètes ou one-hot qui prévalaient jusque-là), où plusieurs dimensions contribuent simultanément à sa signification. Les vecteurs denses représentant les mots capturent la similarité sémantique de ces entités ainsi que des informations syntaxiques dans un espace vectoriel de dimension réduite. Ces représentations vectorielles et leur propriété permettent une généralisation du modèle plus efficace que celle basée sur la co-occurence de mots dans le corpus des modèles n-gramme et apportent ainsi une réponse au problème de la malédiction de la dimensionnalité (curse of dimensionality) (Bengio et al. 2003). L'architecture bipartite proposée par les auteurs prend en entrée une fenêtre de texte de taille fixe, considérant chaque segment textuel de façon indépendante. Il inclut ensuite une couche cachée qui apprend les représentations vectorielles distribuées des mots et une autre partie modélisant la probabilité des séquences de mots. Les représentations vectorielles distribuées des mots sont apprises par le modèle en même temps que la fonction de probabilité pour les séquences de mots (Bengio et al. 2003).
Bengio et al. (2003) ont posé les concepts fondamentaux pour l'utilisation des réseaux de neurones dans le TAL et l'apprentissage des représentations vectorielles des mots en fonction du contexte. Il fallut toutefois attendre des développements ultérieurs pour mettre au point des méthodes neuronales d'embedding efficaces et utilisables à grande échelle (ChatGPT, discussion du 13.01.2024). Word2vec (Mikolov et al. 2013) est l'une d'elles. Développée une décennie plus tard, elle a été spécifiquement conçue pour un apprentissage auto-supervisé et rapide des plongements de mots à partir de grands corpus textuels. A la différence du modèle de Bengio et al. (2003), qui apprenait les plongements dans le cadre de la tâche de prédiction du mot suivant, l'optimisation réalisée par Word2vec se focalise directement sur la similarité sémantique. D'architecture de type FFN peu profonde, Word2vec se décline en deux modèles : Skip-Gram, qui prédit les mots contextuels à partir d'un mot cible, et CBOW (Continuous Bag of Words), qui prédit un mot cible à partir de mots contextuels. Par opposition aux méthodes précédentes qui utilisaient des informations statistiques globales de co-occurrence, ces modèles sont dits shallow window-based, car ils « learn word embeddings by making predictions in local context windows » (Mundra et al. 2019). Dans les deux cas, l'apprentissage repose sur la projection linéaire des embeddings de mots (sans couche cachée à activation non-linéaire). Cette tâche de vectorisation peut être étendue aux documents (Doc2Vec ; Charniak 2021, p. 86). Bien que Word2vec ne soit pas un modèle de langue au sens strict (prédiction séquentielle des mots), il contribua grandement aux améliorations des tâches de TAL en fournissant une représentation numérique vectorielle des mots comme entrées aux modèles de langage neuronaux.
Si le réseau précurseur du laboratoire de Bengio (Bengio et al. 2003) posa les bases des modèles de langue à architecture neuronale profonde, il souffrait néanmoins de plusieurs limitations inhérentes aux réseaux à propagation avant. Parmi les principales, les FFNs sont incapables de gérer des séquences de taille variable et, au-delà d'une fenêtre de contexte limitée, ne captent pas les dépendances longue distance. Aussi, la taille du modèle augmente avec la taille de la fenêtre et les entrées ne sont pas traitées de façon symétrique (poids différents à chaque entrée) (Hashimoto 2024a). Les méthodes comme Word2vec, bien qu'elles apportent une avancée significative pour la génération d'embeddings, n'adressent pas ces limitations. 
Etant donné leurs succès dans la vision par ordinateur dans les premières décennies du deuxième millénaire, il n’est pas surprenant que les réseaux de neurones convolutifs (CNNs) aient été adoptés par les spécialistes du traitement du langage (Chaubard et Socher 2019). Ils obtinrent de bonnes performances, notamment pour des tâches de classification de phrases (Young et al. 2018). Toutefois, contenant un grand nombre de paramètres entrainables, ils requièrent une quantité importante de données d’entrainement (Young et al. 2018). De plus, ils sont difficiles à entrainer, moins stables et moins flexibles que les réseaux de neurones récurrents (RNNs) (Yin et al. 2017 ; Young et al. 2018). Bien que les CNNs aient pu atteindre des performances compétitives dans certaines tâches (Yin et al. 2017 ; Young et al. 2018), leur importance semble donc moindre dans l’évolution des architectures neuronales dédiées au TAL. Pour cette raison, nous ne les détaillerons pas davantage (pas plus que les modèles hybrides CNNs-RNNs) et nous focalisons directement les RNNs.

1.3.	Réseaux de neurones récurrents (RNNs)

Les réseaux de neurones récurrents (RNNs) sont des réseaux dont le graphe de connexion contient au moins 1 cycle. Alors que les réseaux de neurones à propagation avant tels que proposés par Bengio et al. (2003), tout comme les modèles n-gramme, tiennent compte d'un contexte (mots voisins) de taille limitée, les RNNs lèvent cette contrainte et libère de l'hypothèse des dépendances locales (Yvon 2022). La prédiction du mot suivant se fait de proche en proche en s'appuyant sur un état intermédiaire caché. Par ailleurs, puisqu'ils traitent séquentiellement un élément après l'autre – chacun étant associé à un vecteur numérique de même taille – ils peuvent prendre en entrée des séquences de longueur variable, là où un perceptron nécessiterait que toute la séquence lui soit passée en une fois. Dans cette architecture, les poids des connexions entre neurones (paramètres du modèle) sont donc partagés (réutilisés) à chaque étape temporelle. Le partage des paramètres assure que leur nombre reste réduit et constant quelle que soit la longueur de la séquence, et améliore la capacité de généralisation et d’apprentissage des dépendances à longue distance en traitant les entrées de manière symétrique (Mohammadi et al. 2019 ; Gallego et Rìos Insua 2022). Cette architecture cyclique convient dans les situations où des données antérieures influent sur les données suivantes de façon relativement lointaine telles que l'analyse de séquences ou de séries temporelles ou spatiales. Cette configuration est donc particulièrement adéquate pour des tâches de modélisation du langage.
A noter que dans leur conception les RNNs se rapprochent de modèles non-neuronaux déjà utilisés pour le TAL. Par exemple, ils partagent plusieurs similarités avec les modèles de Markov cachés (HMM). Tous deux sont des modèles de graphes dirigés, basés sur des états cachés pour déterminer des probabilités d'émissions (ChatGPT, discussion du 13.01.24). Ils se différencient toutefois puisque l'état caché d'un RNN, calculé sur la base de l'état caché précédent et de l'observation actuelle, est déterminé selon une fonction déterministe. Dans un HMM, c'est une distribution de probabilités qui sert à déterminer (aléatoirement donc) l'état caché actuel en se basant sur les probabilités de transition (paramètres du modèle) et sans dépendre directement de l'observation actuelle (celle-ci influe indirectement, par le biais des probabilités d'émission - autres paramètres du HMM). Les HMMs ne capturent pas les dépendances longue distance bidirectionnelles et souffrent de la malédiction de la dimensionalité (curse of dimensionality) puisque le nombre d’états possibles augmente exponentiellement avec la taille du contexte (Dey 2023).
En fonction du type de transformations désirées, les RNNs peuvent revêtir diverses configurations, en many-to-one (e.g. classification de texte), one-to-many (e.g. génération de texte) ou many-to-many (e.g. traduction automatique), ce qui leur confère une aptitude à accomplir une grande diversité de tâches. Mais tous ont en commun qu’à chaque pas de la séquence, un état caché, conditionné par les états cachés précédents, est mis à jour et transmis d'une position à l'autre. Cette mise à jour peut être effectuée selon différentes modalités. Quelques-unes d’entre elles sont exemplifiées ci-après.
1.3.1.	RNNs simples (vanilla RNNs ou RNNs de Elman)
Par contraste avec les réseaux de propagation avant (graphe acyclique orienté), les RNNs sont caractérisés par une structure où les sorties reviennent au point d'entrée dans le réseau (graphe cyclique orienté) (Charniak 2021, p. 76). Ainsi, au début de la phase de propagation la sortie du réseau de la précédente itération est concaténée avec le plongement courant. Cela leur donne la capacité de garder une "mémoire" des entrées précédentes à travers leur état caché. Pour les modèles de langue, ce vecteur caché est comparé à une banque d'embeddings par un produit scalaire, suivi d'une fonction softmax transformant les scores obtenus en une distribution de probabilités sur tous les mots du lexique en vue de choisir le mot le plus probable (Sagot 2023d).
Pour le reste, le calcul de la passe avant ne diffère pas considérablement des réseaux à propagation avant, appliquant des fonctions d'activation non-linéaires comme tanh ou ReLU pour calculer l'état caché à chaque pas de temps. Pour la phase de rétropropagation, l'ajustement des paramètres ne peut manifestement pas être conduit à l'identique : dans les RNNs, il faudrait remonter l'erreur pour toutes les boucles de récurrence et ajuster les paramètres « grâce aux contributions de tous les mauvais choix depuis le premier mot » (Charniak 2021, p. 77). Dans la pratique, le nombre d'itérations antérieures prises en compte pour la mise à jour des paramètres est défini arbitrairement via un hyperparamètre, la taille de fenêtre. Les phrases sont délimitées par le mot STOP afin que les limites de la fenêtre ne soient pas tenues de coïncider avec celles de la phrase. Ce type de rétropropagation est appelé rétropropagation à travers le temps. Comme déjà mentionné, la projection de la représentation interne donnée par une fonction d'encodage non-linéaire du contexte, calculée de façon récursive, permet d'obtenir la distribution conditionnelle de sortie qui lui est associée, utilisée dans des applications telles que la reconnaissance de la parole, la génération de texte et la traduction automatique (Yvon 2022). L'état initial du réseau avant entrainement est souvent mis à zéro, mais s’actualise à chaque étape pour intégrer l'information des entrées précédentes, reflétant ainsi le flux continu d'information dans les séquences de données.
Tout comme les FNNs, les RNNs peuvent être multicouches (multi-layer ou stacked RNNs) : les représentations produites par une couche de neurones constituent des représentations intermédiaires, données en entrée de la couche suivante (Mohammadi et al. 2019). En augmentant leur profondeur, des representations plus complexes peuvent être apprises : « lower RNNs should compute lower-level features and the higher RNNs should compute higher-level features » (Hashimoto 2024b). Comparée à celle des CNNs ou FFNs, la profondeur des RNNs les plus performants est très modeste car elle se limite généralement à 4 couches tant pour l’encodeur que le décodeur (Hashimoto 2024b).
La structure des RNNs basiques implique que l'influence que peut avoir un mot sur un autre diminue avec l'éloignement, ce qui rend l'ajustement des poids par calcul de gradient (i.e. l'apprentissage) plus compliqué (instabilités numériques ; Yvon 2022). Pour pallier ces problèmes de vanishing ou exploding gradient et permettre de mieux capter les dépendances longue distance, des adaptations à l'architecture des RNNs ont été développées. Celles-ci font recours à un mécanisme de portes qui assiste la régulation du flux d'information le long de la séquence et aide à se focaliser sur l'information importante (Kaylan 2023).
1.3.2.	RRNs à mémoire augmentée - LSTMs et GRUs
Elaborés dans une forme primitive en 1997 déjà (Hochreiter and Schmidhuber 1997), les réseaux de neurones de longue mémoire à court terme (LSTM) ont notamment été développés pour pallier les lacunes des RNNs dans leur faible capacité de mémorisation longue distance. Afin d'améliorer cette dernière, « les LSTM transmettent deux versions du passé : la mémoire sélective "officielle" [dont la chronologie est appelée "état de cellule" - cell state] et une version plus locale » (Charniak 2021, p. 83). La première, un vecteur mis à jour à chaque étape, véhicule les informations à longue distance et représente donc la mémoire à long terme. La deuxième, l’état caché, est constituée de l’information récente (prédiction précédente et entrée courante), formant ainsi la mémoire à court terme. Autrement dit, une cellule LSTM gère une mémoire à court terme tout en maintenant une mémoire à long terme. Et ce via trois portes : d'entrée (input gate; qui sélectionne l'information de l’élément courant à rajouter à la mémoire interne), d’oubli (forget gate ; qui détermine les informations à retirer (i.e. mettre à zéro) de la mémoire interne) et de sortie (output/exposure gate ; qui génère la sortie en prenant en compte l’élément courant et la mémoire interne, elle sépare « the final memory from the hidden state » ce qui actualise la mémoire à court terme) (Mohammadi et al. 2019). Ce mécanisme doit permettre d'éliminer sélectivement de la mémoire les événements du passé peu importants à chaque pas de temps. Il en ressort un état caché ainsi qu'un vecteur de contexte, l’état de cellule (ce que la cellule a gardé en mémoire).
Par ce mécanisme de portes, le LSTM mélange les différentes mémoires et les entrées. Le résultat de l'itération précédente est concaténé au plongement de l'unité lexicale courante avant de passer par une première étape produisant un « signal oublieux » (multiplication terme à terme de la mémoire avec la sortie d'une sigmoïde - bornée entre 0 et 1 ; un exemple de filtrage logiciel ou soft gating). Dans la partie additive, le plongement de mot traverse deux couches linéaires, l'une à fonction d'activation sigmoïde, l'autre tanh. Cette dernière, prenant des valeurs entre -1 et 1, permet l'extraction de nouvelles informations (Charniak 2021, p. 85). Les résultats de ces deux opérations sont multipliés terme à terme et ajoutés à l'état de la cellule. La trajectoire de la ligne mémoire se scinde : une des branches passe par une fonction tanh avant d'être « combiné[e] avec une transformation linéaire de l'historique/plongement le plus local » (Charniak 2021, p. 85), et finalement concaténée avec le plongement du mot suivant à l'itération suivante. Ainsi, durant tout le processus « la ligne de mémoire de cellule ne passe jamais directement au travers d’unités linéaires » (Charniak 2021, p. 85). En résumé, le fonctionnement des LSTMs repose sur deux couches cachées, l'une contrôlant ce qui est retiré de la ligne de mémoire par multiplication d'un vecteur de nombres compris entre 0 et 1 (oubli), l'autre ce qui lui est ajouté par addition d'un vecteur de même type ; ainsi que sur trois portes (d’entrée, d’oubli, de sortie) qui y régulent le flux d’information. Les LSTMs ont représenté l’état de l'art du TAL avant l'arrivée des Transformer (Sagot 2023d). Leur architecture est toutefois d'une complexité certaine et ils sont gourmands en mémoire (Zhang et al. 2023). Ils ne résolvent par ailleurs pas totalement le problème de circulation de l’information (explosion ou disparition du gradient) et ne permettent pas un passage à l’échelle en raison d’un entrainement difficilement parallélisable en raison de la séquentialité des opérations et leur profondeur dépasse ainsi rarement 8 couches (Sagot 2023d).
Avant de conclure cette partie sur les RNNs, mentionnons encore les réseaux à portes récurrents (GRU), une autre version "améliorée" des RNNs, qui, contrairement aux LSTMs, se base sur une seule « ligne de mémoire » (Charniak 2021, p. 90). Se passant d’une output gate explicite, les GRUs sont constitués de deux portes plutôt que trois (Mohammadi et al. 2019). L'une dite de réinitialisation (reset gate) se charge de déterminer si l’information passée doit être oubliée, l'autre dite de mise à jour (update gate) est responsable des informations qui seront transmises plus loin (quelle quantité d’information passée conserver, quelle quantité d’information nouvelle ajouter). Avec une seule porte de mise à jour, le modèle ne différencie pas l’état de la cellule de l’état caché et expose l’entièreté de l’état caché puisqu’aucune porte ne lui spécifiquement dédié (Arkhangelskaya et Nikolenko 2023). Ceci implique un nombre moindre de paramètres et, partant, un entrainement facilité. Bien que nécessitant un entrainement plus long, LSTMs et GRUs (entre autres variantes de RNNs) induisent une amélioration de la performance des modèles neuronaux récurrents (Kaylan 2023). 
En résumé, les RNNs permettent de tenir compte des informations antérieures de la séquence théoriquement de manière illimitée (Mohammadi et al. 2019) et, dans la pratique, de modéliser les dépendances entre les éléments jusqu’à une certaine distance (200 tokens selon Cao et al. 2023). Avec des séquences d’entrée pouvant être de taille variable, ceci représente des améliorations majeures par rapport aux réseaux à propagation avant ou aux modèles n-grammes. Aussi, la taille du modèle n’augmente pas avec la longueur de la séquence (Mohammadi et al. 2019). Toutefois, ils sont difficiles à entrainer et affichent de mauvaises performances pour capturer les interdépendances entre éléments éloignés (Mohammadi et al. 2019). Des variantes (LSTMs, GRUs, etc.) apportent une réponse (limitée) à ce problème, elles sont néanmoins coûteuses car font appel à davantage de paramètres et à des structures internes plus compliquées. Et si la sophistication des modèles récurrents a permis l'amélioration de la capacité de mémoire du réseau de neurones, elle n'abolit pas une autre contrainte : une séquence doit traverser le réseau mot après mot, aussi bien pour l'entrainement qu'en phase de prédiction, et le calcul de la passe courante repose sur les valeurs calculées à la passe précédente (modèle autorégressif ; Yvon 2022). Ceci rend impossible toute parallélisation - particulièrement précieuse en phase d'apprentissage.

C'est sur la base d’un LSTM que furent proposés pour la première fois des modèles dits séquence-à-séquence (seq2seq) par Google en 2014 (Sutskever et al. 2014). Deux RNNs sont mis en action : la représentation latente apprise par le premier (encodeur) sert d'entrées à un deuxième qui génère la séquence (décodeur). Particulièrement utilisé pour la traduction automatique, cet apprentissage séquence-à-séquence (many-to-many) met en correspondance une séquence de symboles avec une autre séquence de symboles, plutôt que d'associer individuellement les symboles (Charniak 2021, p. 89). Diverses tâches de TAL peuvent être formulées comme une modélisation seq2seq : résumé de textes, dialogues, génération de code, etc.. (Hashimoto 2024b). A la différence de réseaux de neurones de type perceptrons multicouches, il n'est plus requis que la séquence de sortie (qui peut être de longueur variable) produite soit de même longueur que la séquence d'entrée (de longueur variable également) (Kaylan 2023). L’entrée comme la sortie peut donc être de longueur arbitraire, bien qu’il soit connu que la performance décline pour de très longs inputs en raison des limitations des RNNs (Genthial et al. 2019). Une autre avancée permise par les modèles seq2seq “especially with its use of LSTMs, is that modern translation systems can generate arbitrary output sequences after seeing the entire input” (Genthial et al. 2019). Dans un modèle de ce type, un premier réseau (composé de LSTMs ou autres variations de RNNs), l'encodeur, produit un vecteur de taille fixe qui représente un plongement de phrase contextualisé (phase d'encodage). L'état final de l'encodeur (ou la somme (ou autre agrégation) de tous les états selon certaines variantes ; Charniak 2021, p. 95) est transmis comme point d'entrée au décodeur. Dans cette deuxième phase, dite de décodage, le but est de prédire le mot suivant dans le langage cible. La couche finale retourne donc un vecteur donnant la probabilité d'occurrence à la position correspondante de chaque item du vocabulaire cible. Des corpus parallèles sont donc nécessaires pour la phase d'apprentissage, qui se réalise comme d'accoutumée par optimisation de la fonction de perte (généralement l'entropie croisée) (Charniak 2021, p. 91). Lors de la phase d'inférence, la sortie de l’unité précédente est utilisée comme entrée dans l’unité suivante et la génération de texte se termine lorsque le décodeur prédit le token de fin de séquence. En plus de leur application initiale pour la traduction automatique (neural machine translation), les modèles seq2seq ont été abondamment utilisés pour d'autres tâches de TAL, notamment comme chatbots (jusqu'à l'arrivée des Transformers).
En traitant les séquences en respectant leur ordonnancement temporel (i.e. l'ordre des mots dans la phrase), un RNN simple n'a pas accès à l'information future (i.e. les mots à droite du mot cible). Cette restriction est levée avec les RNNs bi-directionnels, une disposition dans laquelle deux modèles indépendants parcourent les séquences de texte dans les deux sens (de gauche à droite et inversement) et dont les sorties sont concaténées après la couche de sortie. Puisqu’il maintient ainsi deux couches cachées (une pour la propagation de gauche à droite, une pour celle de droite à gauche), un tel réseau requiert toutefois le double de place en mémoire pour les paramètres de poids et de biais (Mohammadi et al. 2019). Requérant une séquence d’entrée dans son entier, les RNNs bi-directionnels ne sont pas utilisables comme modèles de langue où seul le contexte de gauche est disponible (Hashimoto 2024b). En particulier, les LSTM bi-directionnels (BiLSTMs) ont été utilisés avec succès à des fins de pur encodage (génération de plongements lexicaux enrichis). Cette tâche, ainsi affranchie du besoin de corpus annotés, peut donc s'appuyer sur une quantité faramineuse de données. Ces représentations latentes contextuelles sont utilisées ensuite comme entrées pour d'autres tâches de TAL, pour lesquelles les corpus disponibles sont potentiellement plus limités. Le BiLSTM ELMo (Peters et al. 2018) a notamment marqué une étape importante dans l'évolution des grands modèles de langue en catalysant l'adoption de deux concepts qui domineront les modèles ultérieurs (Guimarães et al. 2024). Tout d'abord, l'apprentissage d'embeddings contextuels profonds là où Word2vec ou GloVe ne fournissaient que des représentations statiques (un mot associé à une représentation vectorielle unique, indépendamment du contexte d'utilisation). Prenant en entrée des vecteurs qui encodent la séquence de caractères de chacun des tokens, il se passe en outre d'un vocabulaire prédéterminé de même d'un découpage en sous-mots, ce qui est particulièrement adéquat pour traiter les mots rares ou absents du corpus d'apprentissage (Sagot 2023d). Le déroulement de l'apprentissage ensuite : un pré-entrainement sur un corpus massif de données non étiquetées, suivi d'un affinement sur des tâches spécifiques avec un corpus plus restreint sans mise à jour des paramètres du modèle pré-entrainé, qui sont dits "gelés" (Yvon 2022). 
Un autre concept développé à l’ère des RNNs est celui de l’attention. Celle-ci fut mise en avant par Bahdanau en 2014 (Bahdanau et al. 2015) à des fins de traduction automatique neuronale avec un RNN séquence-à-séquence constitué d’unités similaires aux LSTMs. Dans ce contexte, l’attention permet au décodeur de « regarder » de façon pondérée les vecteurs produits par l’encodeur, résultant dans une sorte d’alignement entre la phrase d’entrée et celle de sortie (facilitant l’appariement entre un mot français et son équivalent en anglais par exemple). Divers autres modèles d’attention ont par la suite été présentés (Genthial et al. 2019). Entre autres fut établi peu de temps après le principe d’intra-attention (popularisé plus tard sous le terme d’auto-attention ; Cheng et al. 2016) pour modéliser les dépendances à l’intérieur même de l’encodeur et du décodeur.
Plusieurs concepts initiés ou approfondis avec les RNNs (la configuration encodeur-décodeur, le pré-entrainement, les plongements contextuels et l’attention) serviront de pierres angulaires aux architectures ultérieures pour faire progresser le traitement automatique neuronal des langues.

1.4.	Transformer

A l’heure où les modèles génératifs de type VAE ou GAN atteignaient des performances remarquables pour la synthèse d’images (Wang et al. 2020) et connaissaient également des applications pour la génération de texte (Dasgupta et al. 2023), l'arrivée des Transformers en 2017 (Vaswani et al. 2017) marqua un véritable tournant dans le domaine de l’apprentissage profond. Introduit à des fins de traduction automatique, il existe désormais des implantations efficaces des Transformers appliquées à des tâches et modalités les plus diverses (traitement de sons, images, etc.). Nous nous contenterons ici à leur utilisation sur des données textuelles.
Le but principal du Transformer consiste à représenter des données séquentielles en tenant compte de leur contexte, comme dans un RNN, mais sans que cela ne dépende d'un mécanisme de récurrence (i.e. la représentation du mot précédent n'est pas requise pour cette opération). En l’absence d'empilement temporel des états cachés, les calculs peuvent être parallélisés et, partant, une réduction des temps d'entrainement et des performances améliorées. Par ailleurs, alors que dans les RNN l'importance que les mots voisins peuvent se voir attribuer diminue avec la distance, les Transformers exploitent le mécanisme d’attention (décrit ci-après) pour lever cette limitation. Grâce à celui-ci, ils sont en effet capables de considérer de façon égalitaire tous les éléments constitutifs de la séquence : la représentation d'un mot à une position donnée de la séquence d’entrée prend en compte les représentations de toutes les positions de la séquence, captant ainsi le contexte global. 
Forts de ces deux innovations clés, les Transformers ont rapidement supplanté les RNNs pour de nombreuses tâches de TAL (Raiaan et al. 2024). Yvon (2022) en identifie principalement trois : la fouille de texte et de recherche d'information, l'analyse linguistique, et la génération de texte. Chacune d'elles tirent profit de différentes capacités de ce type de réseau de neurones - respectivement, « la richesse des représentations internes [, leur capacité] à prendre en compte des dépendances à très longue distance [, ] leur capacité prédictive ». L'auteur n'hésite d’ailleurs pas à dépeindre le Transformer comme le "couteau suisse" des linguistes. D'autant plus que s’ajoutent à cela d’autres avantages que nous découvrirons au fil des paragraphes qui suivent et décrivent l'un après l'autre les principaux composants du Transformer séminal (Vaswani et al. 2017). 

1.4.1.	Vecteurs d’entrée
L'encodeur comme le décodeur attendent en entrée les représentations vectorielles des mots. Ainsi de nombreuses étapes de préparation de texte (nettoyage, filtrage, etc. – non passées en revue ici) sont nécessaires à la conversion des données textuelles en vecteurs d'entrée pour les modèles. Bien que nous saisissions l’opportunité de le mentionner ici, ceci n’est pas propre au modèle Transformer.
Une étape importante de ce « pré-traitement » est la tokenisation. Selon la méthodologie utilisée, un « token » peut être un mot, un sous-mot, ou un caractère. Par exemple, les modèles GPT-2, RoBERTa ou BART utilisent l'encodage byte-pair (Byte Pair Encoding ou BPE ; Hugging Face 2022). Proposé en 1994 comme méthode de compression de données, elle procède ainsi : en commençant par un ensemble de symboles de base (par exemple, l'alphabet), les paires de tokens consécutifs fréquents dans le corpus sont combinées (fusionnées) de manière itérative pour former de nouveaux tokens. Pour chaque fusion, le critère de sélection est basé sur la fréquence de co-occurrence de deux tokens contigus : la paire la plus fréquente est sélectionnée. Le processus de fusion continue jusqu'à atteindre la taille prédéfinie. Pour sa part, BERT utilise l'algorithme de tokenisation WordPiece et ajoute deux tokens spéciaux ([CLS] et [SEP] en début et fin de phrase). WordPiece est un algorithme mis au point par Google pour des systèmes de recherche vocale (Khanna 2021). ChatGPT nous explique que cet algorithme diffère de BPE car il vise à optimiser "un critère légèrement différent, en se concentrant sur la minimisation de la probabilité de la perte sur l'ensemble de données lors de la formation des tokens. Cela permet à WordPiece de mieux gérer les mots rares ou inconnus en les décomposant en sous-unités plus significatives. De plus, WordPiece commence avec un vocabulaire de base plus riche, incluant non seulement des caractères individuels mais aussi des sous-chaînes de caractères fréquemment observées, ce qui facilite une tokenisation plus efficace et granulaire" (ChatGPT, GPT-4 ; conversation du 20.02.2024).

1.4.2.	Encodage positionnel (positional encoding)
Si l'absence de séquentialité dans le traitement du texte par le Transformer permet son traitement en parallèle, ceci entraine également une perte d'information sur leur position dans la séquence. Pour pallier à cet effet indésirable, plusieurs techniques d'encodage positionnel (soit appris soit statique s’il est calculé par une fonction déterministe) ont été mises au point afin de pouvoir intégrer (généralement par simple addition de vecteurs) les indices positionnels aux vecteurs d'entrée (Yvon 2022). Dans leur l'étude, Vaswani et son équipe ont obtenu des résultats similaires que l'encodage soit basé sur des fonctions trigonométriques ou appris durant l'entrainement (Vaswani et al. 2017). Après l'encodage positionnel, le vecteur contient donc pour chaque token une information sur sa signification et sur sa position dans la séquence qui aide le modèle à être plus précis grammaticalement et sémantiquement (Raiaan et al. 2024). Une autre alternative consiste à inclure dans le calcul de l'attention un terme intégrant la notion de position relative pour tenir compte de l'éloignement entre deux mots (Press et al. 2022).
1.4.3.	L'attention
L'idée de base derrière les mécanismes d'attention réside dans l'hypothèse que certains segments de la séquence source sont plus informatifs que d'autres pour prédire la séquence cible. L'implémentation proposée par Vaswani et ses collègues (Vaswani et al. 2017) dite scaled dot-product attention, diffère de celle précédemment usitée avec les modèles RNN seq2seq. Plutôt que de s'appuyer sur des états cachés séquentiels pour transmettre l'information à travers le temps, cette méthode utilise des matrices pour représenter les requêtes, les clés et les valeurs. Elle partage certains traits communs avec la recherche d'information dans une base de données (Koubaa et al. 2023 ; Poupart 2019) : le mécanisme d'attention détermine l'importance relative de chaque valeur en fonction de la correspondance entre la requête et les clés. Sous cet angle, une base de données contient un ensemble de Valeurs (par exemple des articles de journaux) auxquelles sont associées des Clés (par exemple des mots-clés résumant chacun des articles). Dans cette analogie, lorsqu'une Requête (recherche d'un article de journal) est soumise, le moteur de recherche compare la Requête aux Clés (grâce à une fonction de similarité) et retourne la Valeur (l'article de journal présent dans la base de données) associée à la plus forte similarité. Pour sélectionner une unique valeur de retour, la fonction de similarité produit dans cette exemple un vecteur one-hot" contenant un 1 pour l'article maximisant le critère de recherche, autrement 0. 
Dans le cas du Transformer, nous cherchons à confronter un mot (la Requête) à tous les autres mots de la séquence (les Clés). La fonction de similarité doit permettre d'évaluer l'intensité de l'attention que le mot doit porter aux autres mots de la séquence. Plutôt que de retourner uniquement le mot qui maximise cette similarité, une pondération est ici effectuée : la fonction de similarité produit des scores d'attention qui ne sont pas des vecteurs one-hot mais des valeurs comprises entre 0 et 1 qui permettent d'accentuer les informations les plus pertinentes par multiplication avec les Valeurs (ce qui est retourné de la base de données) correspondant aux autres mots de la séquence (comme si le moteur de recherche renvoie une liste d'articles pondérée par leur pertinence). 
Ces différentes étapes se produisent simultanément pour tous les mots de la séquence par calcul matriciel. Avant le calcul de l'attention, des projections linéaires des embeddings des mots sont calculées en multipliant ces derniers à 3 matrices de poids (paramètres du modèle appris durant l'entrainement) de manière à générer les matrices de Requêtes (Q), Clés (K) et Valeurs (V), où chaque ligne représente un token. La taille (nombre de colonnes) des matrices de poids (et donc des matrices Q, K, V) est un hyper-paramètre du modèle, mais elle est généralement choisie de telle manière que l'espace de projection soit de dimension réduite par rapport à celui de l'embedding initial. Les vecteurs dans la matrice des Requêtes sont confrontés à ceux de la matrice des Clés par produit scalaire (mesure de similarité entre vecteurs qui tient compte de leur direction et amplitude). Ces valeurs sont ensuite divisées par la racine carrée de la dimension d'un vecteur Clé (réduire la variance évite la saturation de la fonction softmax et stabilise le gradient). Puis, le résultat de la division passe par une fonction softmax de façon à être normalisé et correspondre à une distribution de probabilités (tous positifs, ils somment à 1). Ces valeurs représentent les scores d'attention. Pour un mot donné, le mot ayant le score d'attention le plus élevé correspond au mot le plus informatif pour contextualiser le sens de ce mot dans la séquence. L'étape suivante consiste à multiplier ces scores à la matrice des Valeurs. L'intuition ici est de maintenir la valeur des mots importants (en les multipliant par des valeurs proches de 1) tout en rapprochant celle des autres à 0 (en les multipliant à des valeurs proches de 0) afin de permettre au modèle de se concentrer sur les parties les plus pertinentes des données. Finalement, il reste à agréger ces vecteurs de valeurs pondérées en les sommant afin de produire la sortie de la couche d'attention, qui fournit ainsi pour chaque mot un vecteur de représentation contextuellement enrichie.
Dans le Transformer, l'attention est dotée d'une sophistication supplémentaire : elle est multi-têtes. En bref, l'attention multi-têtes applique le mécanisme d'attention décrit ci-dessus plusieurs fois (8 dans Vaswani et al. 2017) de manière simultanée, chacune avec des transformations de clés, de requêtes et de valeurs différentes de la même entrée, avant de combiner les sorties de chaque tête. Cet agencement a pour but de capter des motifs de dépendance plus complexes en saisissant simultanément plus de caractéristiques et relations dans les données. Cela peut servir par exemple à résoudre les résolutions pronominales (Yvon 2022). 
L'attention peut (doit) également être masquée. Le masquage consiste à introduire des valeurs extrêmement basses dans la matrice de scores d'attention avant l'application de la fonction softmax à certaines positions spécifiques. De la sorte, les positions masquées se voient attribuer des poids d'attention de zéro ce qui les rend "invisibles" (i.e. sans influence sur le processus de prédiction des mots) (Alammar 2018). Dans un Transformer, il y a généralement de deux sortes de masquage : de remplissage (padding mask) et d'anticipation (look-ahead mask). Le premier, davantage une mesure technique, est utilisé pour éviter que le mécanisme d'attention considère les tokens de remplissage ("<pad>") ajoutés pour s'assurer que les séquences d'un même lot (batch) soient de même longueur lors de l'entrainement. Le deuxième, essentiel pour la génération de texte, vise à empêcher le modèle de "tricher" lors de l'entrainement, en lui laissant prêter attention qu'aux mots qui le précèdent, de façon à imiter la génération de mot de façon séquentielle au moment de l'inférence.
Enfin, l'attention peut être croisée (cross-attention) ou auto-dirigée (self-attention). Le premier cas correspond à la situation où chaque mot de la séquence cible, dans le décodeur, "regarde" et intègre des informations sur la séquence d'entrée produite par l'encodeur. Elle est similaire à l'attention déjà décrite dans les modèles RNN de type séquence-à-séquence et permet en quelque sorte d'aligner la séquence de sortie sur la séquence d'entrée. L'auto-attention a lieu, dans l'encodeur et le décodeur, lorsque la séquence (d'entrée ou de sortie respectivement) se regarde elle-même dans le but d'apprendre les relations au sein de la séquence elle-même.
Cette formulation de l'attention dans une étape de calculs matriciels "simples" remplace efficacement, pour le traitement du langage tout du moins, les architectures neuronales récurrentes ou convolutives plus difficiles à entrainer et sujettes aux problèmes de disparition ou explosion de gradients. De surcroit, l'attention de Vaswani et al. (2017) permet une plus grande efficacité computationnelle puisque toutes les opérations peuvent être effectuées en parallèle. De plus, elle favorise une meilleure compréhension du contexte global et des dépendances à longue distance. L'utilité de l'attention des Transformer ne se limite à pas à son application à des données textuelles. Dans l'analyse d'images, elle permet par exemple de porter attention spécifiquement sur certains objets qui les composent. En plus de l’implémentation originelle exposée ci-dessus basée sur le produit scalaire, il existe diverses variantes de l’attention (additive, multiplicative, etc. ; Hashimoto 2024c).
1.4.4.	Réseau à propagation avant entièrement connecté par position
La sous-couche du réseau à propagation avant (FFN) dense qui suit celle d'attention fait intervenir successivement : une première transformation linéaire, une activation non-linéaire (typiquement ReLU), une deuxième transformation linéaire. La transformation linéaire consiste en la multiplication par une matrice de poids et l'ajout d'un biais (i.e. un neurone ajoutant un poids ajustable et prenant toujours 1 comme valeur d'entrée ; Vannieuwenh 2019, p. 285) : la première augmente la dimensionnalité de l'embedding ; la deuxième le ramène à sa taille à la sortie de la couche d'attention. Cette séquence d'opérations, qualifiée de position-wise, est appliqué à chaque embedding de token séparément et indépendamment des autres éléments de la séquence (bien que les paramètres soient partagés), équivalant à l'effet d'une convolution 1x1. Puisque la couche d'attention en est dépourvue, la non-linéarité apportée par la fonction d'activation intermédiaire est essentielle pour la capacité de modélisation du modèle.
1.4.5.	Connexions résiduelles et normalisation ("Add & Norm")
Les sorties de la couche d'attention sont combinées avec les sorties de la couche précédente. Ces connexions résiduelles permettent de conserver les informations originales et aident à stabiliser le gradient en offrant une voie parallèle sans fonction d'activation pour un meilleur entrainement (Hashimoto 2024c). Pour chaque chaque token de la séquence, le résultat de l'addition est ensuite normalisé (layer normalization), en soustrayant la moyenne et divisant par l'écart-type (standardisation) afin de « cut down on uninformative variation in hidden vector values » (Hashimoto 2024c). Si la normalisation intervient après l’ajout des connexions résiduelles, on parle alors de post-normalisation. Dans certaines implémentations, la normalisation est appliquée avant l'ajout des connexions résiduelles (pré-normalisation) (Hewitt 2023). Cette étape contribue à accélérer l’apprentissage (Hashimoto 2024c). Ajuster ainsi les activations à une distribution normale standard aide principalement à atténuer les problèmes de disparition ou explosion de gradients et stabilise l'entrainement (Kaylan 2023).
Après avoir vu les différentes briques du Transformer, voyons comment elles s'assemblent dans les deux blocs que sont l'encodeur et le décodeur dans le modèle proposé par Vaswani et al. (2017).
1.4.6.	Partie encodeur 
Après avoir ajouté une information structurelle en additionnant un plongement positionnel à la représentation numérique vectorielle du texte en entrée, la séquence d'entrée traverse n couches (n=6 dans l'article original) de construction identique : une sous-couche d'auto-attention multi-têtes (8 dans l'article original), suivie par un réseau à propagation avant (FFN) entièrement connecté. Chaque sous-partie de la couche intègre une connexion résiduelle, c'est-à-dire que le résultat de chaque sous-partie est additionné à son entrée, avant d'être normalisé.

1.4.7.	Partie décodeur
L'architecture causale du décodeur est très similaire à celle de l'encodeur, hormis deux adaptations majeures. Dans chacune des couches, une sous-couche d'attention croisée (également multi-têtes) est insérée entre la normalisation qui suit l'auto-attention et le FFN. Cet ajout permet au décodeur d'incorporer des informations de la séquence source pendant la génération de la séquence cible. Ensuite, l'auto-attention dans cette partie du Transformer est masquée afin d'assurer une génération de texte autorégressive (le look-ahead masking cachant les tokens futurs). 
Après la sortie de la normalisation par couche qui fait suite au réseau à propagation avant, les sorties du décodeur passent par une couche linéaire de projection qui ajuste leur dimension à la taille du vocabulaire (ou au nombre de classes si le modèle est entrainé en vue d'une tâche de classification). Grâce à cette transformation affine, un vecteur comportant un score (logit) pour chacun des mots est ainsi obtenu. Il ne reste qu'à appliquer à scores une fonction softmax pour les convertir en une probabilité proportionnelle à leur exponentielle. Le mot associé à la probabilité la plus haute correspond au mot à ajouter à la séquence en cours de génération. À chaque étape de génération de texte lors de l'inférence, les mots produits sont réintroduits en entrée du décodeur, permettant ainsi la construction séquentielle de la sortie. A l'entrainement, le teacher forcing décale la séquence vers la droite avant de la fournir au décodeur pour que la prédiction se base uniquement sur les mots précédents.
Une subtilité supplémentaire, qui n’est pas propre aux Transformers, s'ajoute au processus de génération de texte. En effet, si seul le mot le plus probable était choisi en sortie (greedy search ou greedy decoding), des phrases de probabilité très proche seraient d'emblée écartées et « if we make a mistake at one time step, the rest of the sentence could be heavily impacted » (Genthial et al. 2019). Sans considérer d'autres variantes, le texte entier obtenu en fin de compte pourrait manquer de cohérence et/ou d'optimisation globale. Une méthode souvent utilisée est la recherche en faisceaux ou beam search, qui consiste à conserver à chaque étape de la génération, les k solutions les plus probables (k=1 équivaut à la recherche greedy ou gloutonne). Par exemple pour k=2 : à la première position, les deux tokens les plus probables sont conservés ; le modèle est exécuté séparément pour chacun de ces tokens ; après les deux exécutions du modèle, sur l'ensemble des séquences produites on ne garde à nouveau que les deux associées aux probabilités les plus élevées ; pour chacune de ces deux séquences, le modèle est à nouveau exécuté, et ainsi de suite (Doshi 2021). Des valeurs de k plus élevées améliorent (de façon non monotone) la précision et k peut être choisi de façon à combiner performance acceptable et efficacité computationnelle. De ce fait, cette méthode est la plus utilisée pour les systèmes de traduction neuronale (Genthial et al. 2019). Les valeurs de k sont généralement choisies entre 3 et 6 (4 pour GPT-3 ; Zhao et al. 2023). D'autres méthodes permettent d'influencer le choix des mots et rendent la génération de texte plus modulables. Un paramètre fréquemment utilisé est la température, un coefficient intégré au dénominateur de l'exponentielle dans la formule softmax pour ajuster la distribution de probabilités des tokens. Une valeur égale à 1 reste sans effet sur la distribution de probabilités. Lorsque les valeurs tendent vers zéro, la distribution des probabilités devient de plus en pointue, se concentrant autour des tokens les plus probables. A valeur nulle, la sélection est entièrement déterministe et équivaut à une recherche gloutonne. A l'inverse, plus les valeurs dépassent 1, plus la courbe s'aplatit et s'uniformise : l'incertitude sur le choix du token s'accroit ; le texte généré est plus diversifié, le modèle plus "créatif".

1.4.8.	Encodeur et/ou décodeur ?
L'architecture encodeur-décodeur permet de prédire du texte mot par mot en fonction de l'entrée. Une configuration particulièrement adéquate pour les tâches de traduction, scénario présenté dans l'article fondateur de Vaswani et al. (2017). D'autres tâches de conversion texte-à-texte dans lesquelles ils performent consistent par exemple à corriger, résumer ou paraphraser un texte de même que générer une réponse à une question. Un autre exemple est BART (Lewis et al. 2020). Ce modèle seq2seq qui combine encodeur bidirectionnel et décodeur autorégressif, est entrainé à reconstruire la phrase d'entrée bruitée. Proposé plus récemment par Google (Raffel et al. 2020), T5 a été entrainé sur plusieurs tâches supervisées ou non. 
Par sa modularité, l'architecture Transformer offre une flexibilité suffisante pour utiliser l'encodeur ou le décodeur de façon indépendante en fonction du but recherché. Un modèle encoder-only permet l'apprentissage de représentations riches et contextuelles des données d'entrée. Celles-ci peuvent ensuite être utilisées pour accomplir des tâches de compréhension de langage (i.a. classification de textes, extraction d'entités ou de relations ; (Kaylan 2023). BERT (Bidirectional Encoder Representations from Transformers ; Devlin et al. 2019) en est supposément l'archétype le plus fameux. Selon Bommasani et ses co-auteurs (Bommasani et al. 2021), BERT marqua même « a sociological inflection point », ce modèle étant devenu la norme pour les tâches de TAL et annonce l'entrée dans l'ère des modèles de fondation. Ce modèle de langue par masquage présenté en 2018 par Google AI marque une avancée notable dans le domaine du TAL pour la représentation profonde bidirectionnelle des mots (i.e. prenant en compte le contexte antérieur (gauche) et postérieur (droite)). Avec plus de 300 millions de paramètres pour sa version la plus grande (BERT-large : 24 couches, 16 têtes d’attention ; Hashimoto 2024d), il a été conçu comme un modèle pré-entrainé de façon non-supervisée sur un corpus de données massif (4 mia de tokens issus de Wikipedia Anglais (2’500 mio de mots) et BookCorpus (800 mio de mots), soit env. 16 GB de texte non compressé ; Hashimoto 2024d, Sagot 2023d). Si le pré-entrainement fut très demandeur en ressources (4 jours de calculs sur 64 chips TPU), le fine-tuning peut être mis en œuvre de façon pratique sur 1 seul GPU selon le principe « pretrain once, finetune many times » (Hashimoto 2024d). Deux tâches sont apprises durant l'entrainement. La première, la prédiction d'un mot masqué, permet de créer une représentation au niveau du token ; la deuxième, la prédiction de la probabilité que deux phrases se suivent ou ne se suivent pas, capte les dépendances entre les phrases (Guimarães et al. 2024). En plus des plongements lexicaux des tokens, BERT utilise les représentations cachées du token spécial [CLS] comme plongement de phrases, utiles par exemple pour des tâches de classification. Il se destine à être affiné en lui adjoignant une seule couche de sortie supplémentaire pour l'entrainer sur des tâches spécifiques (par exemple : étiquetage de tokens ou de phrases, analyse de sentiments) (Devlin et al. 2019). Jusqu'à aujourd'hui, BERT continue d'engendrer de nombreux "descendants" (MultilinguialBERT, couvrant plus de 100 langues ; CamemBERT ou FlauBERT, entrainés sur des données françaises ; etc.) (Sagot 2023d). 
La partie décodeur produit un texte de manière auto-régressive (modèle de langue). Cela sied à des situations où le texte à générer n'a pas besoin d'être directement aligné sur la séquence d'entrée. Les modèles au cœur des agents conversationnels comme ChatGPT sont donc fréquemment de ce type. Ce type de modèles prédominent actuellement les grands modèles de langue (e.g. GPT-4, PaLM, LLaMA ; Zhao et al. 2023). Nous abordons en détail les modèles GPT au chapitre suivant.
1.4.9.	Avantages et limites des Transformer

L’avancée majeure portée par les Transformers et leur mécanisme d’attention est d’avoir rendu possibles, grâce à la façon dont est implémentée l’attention, la parallélisation des calculs (séquences entières traitées en un coup) ainsi que l’augmentation de la portée du contexte (Sagot 2023d). En offrant la possibilité au décodeur de « regarder » directement la séquence source plutôt que de se baser uniquement sur un vecteur de caractéristiques de taille fixe, l’attention solutionne le bottleneck problem des modèles seq2seq antérieurs (Hashimoto 2024c). L’inspection de la distribution de l’attention apporte en outre une certaine explicabilité (Hashimoto 2024c).
L'attention par produit scalaire normalisé ici décrite (de type full attention) n'est pas exempte d'inconvénients. De complexité quadratique par rapport à la longueur de séquence en temps et en mémoire, elle représente une limitation majeure pour le traitement de longues séquences et impacte l'entrainement autant que l'inférence (Yvon 2022). Il y a donc une recherche active en vue d'optimiser cette étape pour réduire la complexité computationnelle et de nombreuses variantes ont été proposées en vue de la rendre linéaire (Sagot 2023d). L'attention éparse (sparse attention), où « instead of the whole sequence, each query can only attend to a subset of tokens based on the positions » (e.g. Factorized Attention dans GPT-3), en est une parmi de nombreuses autres (multi-query attention, FlashAttention, PagedAttention, etc.) (Zhao et al. 2023). Une autre voie de recherche s’attèle à combiner Transformers et RNNs (e.g. RetNet ; Sun et al. 2023) en vue de tirer parti du meilleur de chacune de ces architectures (respectivement : encodage d’une séquence entière en parallèle et décodage plus efficace basé sur l’état précédent uniquement) (Zhao et al. 2023).
Par ailleurs, ces modèles, requérant souvent des ajustements techniques, restent difficiles à entrainer (Sagot 2023d). Tout un pan de la recherche actuelle vise à les améliorer à l’aide de méthodes innovantes autant pour les couches de la normalisation, l'initialisation des paramètres et de nombreux autres adaptations (Zhao et al. 2023 ; Wan et al. 2023). 
Un dernier point d’attention concerne le corpus d’entrainement. Pour atteindre de bonnes performances, l’apprentissage des Transformers actuels doit se baser sur des quantités de données gargantuesques. De surcroit, un équilibre doit être trouvé entre la taille des modèles et la taille (diversité) des corpus, qui doivent croitre en parallèle (Sagot 2023d). Comme illustré par Chinchilla (Hoffmann et al. 2022), le nombre de paramètres ne fait pas tout : avec « seulement » 70 mia de paramètres, ce modèle battait pour de nombreuses tâches d’évaluation nombre de modèles dotés de plusieurs centaines de milliards de paramètres. 

1.4.10.	Transformers, LLMs et changements de paradigmes
Dans une section précédente, nous avons brièvement retracé le développement des méthodes de TAL, des approches symboliques aux réseaux de neurone profonds modernes. A leur début, un apport majeur de ceux-ci fut leur capacité à représenter les mots de façon distribuée et à utiliser ces vecteurs pour la prédiction de texte. De la sorte, à consacrer une utilisation ne se limitant plus à la seule modélisation de séquences mais s'étendant à l'apprentissage de représentations (representation learning ; Zhao et al. 2023). Avec la popularisation des Transformers, d’autres paliers furent franchis dans la modélisation du langage. 
Tout d'abord, le succès des modèles de langue pré-entrainés (PLMs). Déjà usité bien avant les LLMs, le pré-entrainement permet de commencer l'entrainement sur la tâche spécifique avec les valeurs de poids obtenues lors du pré-entrainement plutôt qu'une initialisation aléatoire (Charniak 2021, p. 128). De façon importante, le pré-entrainement peut être mené sur un vaste corpus de données non étiquetées avant de se focaliser sur une tâche spécifique pour laquelle un nombre restreint de données sont disponibles (Yvon 2022). Quand il s’agit de données textuelles, la phase de pré-entrainement permet par exemple au modèle d'acquérir des notions sur le contexte, la grammaire, le vocabulaire de même que des connaissances générales (Hadi et al. 2023). Le BiLSTM ELMo (Peters et al. 2018) y faisait déjà recours et passait au modèle spécifique en aval les représentations apprises dans la phase initiale (feature-based approche) (Devlin et al. 2019). Avec BERT et GPT c’est la combinaison pré-entrainement suivi d’ajustement fin (fine tuning) qui est érigée comme paradigme d'apprentissage (Kaylan 2023 ; Liu et al. 2023a) ; Zhao et al. 2023). Déjà utilisé avec succès en vision par ordinateur, le fine-tuning ou affinement réduit le besoin en données étiquetées à la phase d'ajustement (spécifique à chaque tâche), qui en requiert dès lors une quantité bien moindre, l'acquisition de connaissances générales s'opérant de manière non supervisée lors du pré-entrainement (Kaylan 2023). A noter que cette prévalence du pré-apprentissage non-supervisé a également des implications sur la façon dont se conduit le travail des développeurs : requérant moins d'a priori structurels (structural priors) et étant très gourmand en ressource, les possibilités d'expérimenter des variantes architecturales s'en retrouvent restreintes (Liu et al. 2023a).
Après des études pionnières décrivant les premiers PLMs a suivi une phase de recherche constatant l'amélioration de ces modèles avec la mise à l'échelle des modèles et/ou des données. Ainsi advint l'ère des grands modèles de langue (LLMs; e.g. GPT-3, PaLM, LLaMA) aux capacités surprenantes et capables d'accomplir des tâches complexes. Essentiellement basés sur une architecture Transformer comme les PLMs, ils s'en distinguent par un nombre incommensurable de paramètres (centaine de milliards), de calculs et la quantité astronomique de données utilisées pour l'entrainement (Kaylan 2023 ; Zhao et al. 2023). La frontière entre PLMs et LLMs demeure floue, les valeurs de seuil n'étant pas pour l'heure clairement définies et la terminologie ne semble pas faire l'objet de consensus parmi la communauté scientifique. Comme d’autres auteurs, Raiaan et ses collègues (Raiaan et al. 2024) font remonter le premier grand modèle de langue au système de traduction automatique dévoilé par Google en 2015 (Google's Neural Machine Translation system ; Wu et al. 2016). Bommasani et ses co-auteurs ont regroupé sous le terme de modèle de fondation (foundation model) ce type de modèles généralistes, « trained on broad data (generally using self-supervision at scale) that can be adapted (e.g., fine-tuned) to a wide range of downstream tasks » (Bommasani et al. 2022) et « distinct from narrow AI systems, which are trained for one specific task and context » (Ada Lovelace Institute 2023).
Surtout, les LLMs sont caractérisés par des capacités émergentes (encore mal comprises) qui ne se manifestent pas à plus petite échelle. Parmi celles-ci, Zhao et ses collègues en citent trois : "in-context learning [,] instruction following [and] step-by-step reasoning" (Zhao et al. 2023). Le paradigme de l'apprentissage en contexte (in-context learning) fait référence à "a new learning paradigm that does not require task-specific fine-tuning and many labelled instances" (Kaylan 2023). La mise au point des LLMs inclue généralement une phase d'alignement dite de méta-entrainement (meta-training ; Kaylan 2023) pour conformer les sorties du modèle aux valeurs et préférences attendues des utilisateurs (Kaylan 2023) (voir section 6.1.).
Les LLMs se montrent capables de performances dans (quasiment ?) toutes les tâches de TAL - des questions-réponses à la génération, classification et synthèse de texte, en passant par la traduction, l'extraction d'informations ou la reconnaissance de dialogues. Mais l'objectif des modèles de langue a évolué au fil de ces développements, et ne se limite désormais plus à la "simple" modélisation du langage puisqu’il s'est élargi à la résolution de tâches complexes (Zhao et al. 2023). Liu et ses co-auteurs (Liu et al. 2023a) font remonter à2021 un nouveau tournant qu'il résume à "pre-train, prompt, predict". Plutôt que de spécialiser les modèles pré-entrainés en adaptant les fonctions objectives, les tâches en aval sont formulées au moyen d'une requête textuelle (prompt) de façon à correspondre à celles résolues lors de la phase d'entrainement initiale. De nouvelles pratiques voient le jour en remplacement du fine-tuning, par exemple le few-shot prompting, qui consiste à "includ[e] a few examples selected from the dataset in the prompt, to help the model better understand task requirements" (en l’absence de tout exemple, on parle de zero-shot prompting ; Cao et al. 2023). Ainsi, sans passer par un entrainement spécifique supervisé, leur seul apprentissage non-supervisé permet à ces modèles de résoudre des tâches très variées. 

Au final, l'expansion progressive des capacités des modèles de langue peut ainsi être résumée : les premiers LMs statistiques se restreignaient à des tâches spécifiques (e.g. recherche d'informations), les premiers NNs furent capables d'apprendre des caractéristiques de façon agnostique, de "bout-en-bout" (se passant de feature engineering "manuel"), les PLMs apprirent des représentations contextualisées à optimiser pour les tâches en aval, les LLMs bénéficient de l'effet d'échelle et "can be considered as general-purpose task solvers" (Zhao et al. 2023). En outre, les LLMs s'accompagnent également de bouleversements pratiques : ils sont désormais accessibles par une interface de prompts et requièrent des capacités matérielles et techniques inédites pour être entrainés et maintenus (Zhao et al. 2023).

{% include links.html %}
